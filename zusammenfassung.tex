\documentclass[a4paper]{article} 
\usepackage[intlimits,tbtags]{amsmath}
\usepackage{amssymb}   
\usepackage{amsfonts}
\usepackage{latexsym}  
\usepackage{amsxtra}   
\usepackage{amstext}   
\usepackage{bm}        
\usepackage{amsthm}    
\usepackage{amscd}     
\usepackage[dvipsnames]{xcolor}
\usepackage{marvosym}
\usepackage[utf8]{inputenc}
\usepackage[shortlabels]{enumitem}
\usepackage{array}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\E}{\mathbb{E}}
\newcommand{\hatt}{\hat{\theta}_m}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{hyperref}

\title{Summary of the Deep Learning Lecture at the University of Bonn}


\author{Tim Nogga}

\begin{document}


\maketitle
\tableofcontents
                                                                                                                                
\section{Introduction}
\subsection{Task T}
Machine Learning Tasks describes how a machine learning system learns from data. Such data can be images(pixels) among many. Typical Tasks include Klassification, Regression, and Density Function Estimation.

\subsection{Experience E}
Specifies the Information an Algorithm can use during the Learning Process.

\subsubsection{Supervised Learning}
The Algorithm is given a dataset with the correct answers. The Algorithm then tries to learn the mapping from the input to the output. This can be described with an Estimation $p_data(y|x)$.

\subsubsection{Unsupervised Learning}
The Algorithm is given a dataset without the correct answers. The Algorithm then tries to learn the underlying structure of the data. This can be described with an Estimation $p_data(x)$.

\subsection{Performance Measure P}
Learning Algorithms can be evaluated based on their Performance. In Classification Problems, the Accuracy is a common measure. %TODO: what is a mittlere logarithmische Wahrscheinlichkeits von Samples?
The Perfomance Measure is calculated on a Test Set, which is not used during the Training Process. So there a are two sets a Test Set and a Training Set. 

\subsection{Training Error, Test Error and Generalization}
Generalization is the ability of an algorithm to perform well on previously unseen data. During Training the Training Error should be reduced, but the Test Error should be reduced as well. If the Training Error is reduced but the Test Error is not, the algorithm is overfitting. Meaning the algorithm is learning the noise in the data instead of the underlying structure. But if the Test Error is not even reduced the algorithm is underfitting.

\subsection{Capacity}
Overfitting and underfitting are also dependent on the capacity of the model. Say you take a model with with a low capacity, it will underfit the data, as it is not able to learn the underlying structure. But if a model with high Capacity is used, it will just memorize the data and thus overfit, as it is not able to generalize.

\subsection{Capacity vs Error}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/capacity.png}
    \caption{}
    \label{fig:}
\end{figure}
It becomes apparent that the Training Error decreases with increasing Capacity. But the Test Error decreases first and then increases again. This is due to the fact that the model is overfitting the data, thus not able to generalize anymore. This is also why it is called Generalization Gap.

\subsection{Hyperparameters}
Leraning Algorithms usually have Hyperparameters which are paramters not learned during Training, but set before. These could be things such as Model Capacity and Learning Rate. This poses the Question wether Capacity Hyperparameters can be derived from the Trainingset. The answer is no, since the Learning Algorithm would always choose the highest Capacity, as it would always reduce the Training Error.

\subsection{Cross Validation}
Cross Validation can be used to determine the best Hyperparameters. The Training Set is split into k parts. Then the Algorithm is trained on k-1 parts and tested on the remaining part. This is done k times, so that every part is used as a Test Set once. The average Test Error is then used to determine the best Hyperparameters. 

\section{Recap Statistics}
\subsection{Terms and Definitions}
\begin{itemize}
    \item \textbf{Sample Space} $\Omega$: Set of all possible outcomes of an experiment. For example a dice roll has a Sample Space of $\Omega = \{1,2,3,4,5,6\}$
    \item \textbf{Event} $A$: Subset of the Sample Space. For example the Event of rolling an even number is $A = \{2,4,6\}$
    \item \textbf{Sigma Algebra} Collection of Events. For example the Sigma Algebra of the dice roll is $\sigma(\Omega) = \{\emptyset, \{1,2,3,4,5,6\}, \{1,3,5\}, \{2,4,6\}\}$
    \item \textbf{Measurable space} $(\Omega, \mathcal{A})$: Is a Measurable Space if $\Omega$ is a Sample Space and $\mathcal{A}$ is a Sigma Algebra.
\end{itemize}

\begin{itemize}
    \item \textbf{Probability Density Function if $\Omega$ is Discrete} x $p: \Omega \rightarrow [0,1]$ with $\sum_{\omega \in \Omega} p(\omega) = 1$. 
    \item \textbf{Probability Density Function if $\Omega$ is Continuous} x $p: \Omega \rightarrow [0,\infty)$ with $\int_{\omega \in \Omega} p(\omega) d\omega = 1$.
\end{itemize}

\subsection{Multivariate Statistics}
It just means that there are more than one Random Variable. For example if we have two Random Variables $X$ and $Y$ we can define a Joint Probability Density Function $p(x,y)$.

\subsection{Marginalization in a Discrete Case}
We can get the PDF of one Random Variable by summing over the other. For example $p(x) = \sum_{y} p(x,y)$. or $p(y) = \sum_{x} p(x,y)$.
In the Case of getting $p(x)$ this is just Summing over all possible values of $y$, which is rather Intuitive. 

\subsection{Marginalization in a Continuous Case}
In the Continuous Case we have to integrate over the other Random Variable. For example $p(x) = \int p(x,y) dy$. or $p(y) = \int p(x,y) dx$.

\subsection{Marginalization in a Mixed Case}
In a Mixed case if $p(x)$ is Continuous and $p(y)$ is Discrete we have to sum over $y$ and integrate over $x$. For example $p(x) = \int p(x,y) dy$ or $p(y) = \sum_{x} p(x,y)$.

\subsection{Marginalization in Multiple Dimensions}
If we for example want to know (p(x,y) given 4 parameters x,y,z,w) we have to marginalize over z and w. If w is Discrete we sum over w and if z is Continuous we integrate over z. This results in the following $p(x,y) = \sum_{w} \int p(x,y,z,w) dz dw$.

\subsection{Conditional Probability}
The Conditional Probability is defined as $p(x|y = y_1)$ and is the Probability of $x$ given that $y$ is $y_1$. 
 $p(x|y = y^{*}) = \frac{p(x,y = y^{*})}{p( y = y^{*})}$
 This is rather intuitiv aswell if you imagine that you are only interested in this one case where $y$ is $y^{*}$. And then you will need to devide by the Probability of this case happening, independent of $x$.

This can be rewritten as $p(x|y) = \frac{p(x,y)}{p(y)}$. \\
Which in turn can be rewritten as $p(x,y) = p(x|y) p(y)$ and $p(x,y) = p(y|x) p(x)$.

\subsection{Independence}
Two Random Variables are independent if $p(x,y) = p(x) p(y)$. This means that the Probability of $x$ and $y$ happening is the same as the Probability of $x$ happening times the Probability of $y$ happening.

\subsection{independent and Identically Distributed}
As an example one could think of a Dice Roll. The Dice Roll is independent and identically distributed, as the Probability of rolling a 1 is the same for every roll and the Probability of rolling a 1 and a 2 is the same as the Probability of rolling a 1 times the Probability of rolling a 2.
Formally this can be written $p(x_1, x_2, ..., x_n) = p(x_1) p(x_2) ... p(x_n).$

\subsection{Bayes Rule}
As previously discussed we have $ p(x,y) = p(x|y)p(y) and p(x,y) = p(y|x)p(x)$
By setting these two equations equal we get $p(y|x)p(x) = p(x|y)p(y)$
This can be rewritten as $p(y|x) = \frac{p(x|y)p(y)}{p(x)}$ which is the Bayes Rule.
With the backknowledge that $p(x) = \sum_{y} p(x|y)p(y)$ for discrete  and $p(x) = \int p(x|y)p(y) $ for Continuous cases  we can rewrite the Bayes Rule as $p(y|x) = \frac{p(x|y)p(y)}{\sum_{y} p(x|y)p(y)}$ or $p(y|x) = \frac{p(x|y)p(y)}{\int p(x|y)p(y) dy}$.
Here the Likelihood is the $p(x|y)$, the Prior is the $p(y)$ and the Posterior is the $p(y|x)$ and the Evidence is the $p(x)$. The Likelihood is the the Probability to observe a certain x given a certain y. The Prior is the Probability of y happening. The Posterior is the Probability of y happening given x. The Evidence is the Probability of x happening.

\subsection{Expectation}
The Expectation is the average value of a Random Variable. It is defined as $E[x] = \sum_{x} x p(x)$ for Discrete Random Variables and $E[x] = \int x p(x) dx$ for Continuous Random Variables.

\subsection{Variance and Standard Deviation}

The Variance is a measure of how much the values of a Random Variable differ from the mean. It is defined as $Var[x] = E[(x - E[x])^{2}] = E[x^{2}] - E[x]^{2}$.
The Standard Deviation is the square root of the Variance. It is defined as $\sigma = \sqrt{Var[x]}$.

\subsection{Shannon Entropy}

The Shannon Entropy gives us an answer to the Question how many bits are needed to encode Samples from a Probability Distribution. It is defined as $H(x) = - \sum_{x} p(x) \log p(x) = E[- \log(p(X))]$ for Discrete Random Variables. 
This can be used to measure the Uncertainty of a Random Variable. If the Entropy is high, the Random Variable is uncertain. If the Entropy is low, the Random Variable is more certain.
So for a Coin Flip the Entropy would be $H(x) = - \frac{1}{2} \log \frac{1}{2} - \frac{1}{2} \log \frac{1}{2} = 1$. This means that it takes 1 Bit to encode the Coin Flip. 

\subsection{Cross Entropy}

The Cross Entropy is the same as the Shannon Entropy, but for two Probability Distributions. It is defined as $H(p,q) = - \sum_{x} p(x) \log q(x) = E_p[- \log(q(X))]$ for Discrete Random Variables. This can be used to measure the difference between two Probability Distributions. If the Cross Entropy is high, the Distributions are different. If the Cross Entropy is low, the Distributions are similar.

\subsection{Kullback-Leibler Divergence}

The Kullback-Leibler Divergence is a measure of how much one Probability Distribution differs from another. It is defined as $D_{KL}(p||q) = H(p,q) - H(p) = \sum_{x} p(x) \log \frac{p(x)}{q(x)} = E_p[\log \frac{p(X)}{q(X)}]$ for Discrete Random Variables. This can be used to measure the difference between two Probability Distributions. If the Kullback-Leibler Divergence is high, the Distributions are different. If the Kullback-Leibler Divergence is low, the Distributions are similar, if the Divergence is 0 the Distributions are the same.

\section{Estimators}
\subsection{Point Estimation}

A Point Estimator is a function of the data that is used to estimate an unknown parameter $\hat{\theta}$ This can also be interpreted as an Learning Algorithm, where the Parameter is supposed to be learned. The Point Estimator is denoted as $\hat{\theta}$. 

\subsection{Bias}

The Bias of an Estimator is the difference between the expected value of the Estimator and the true value of the parameter. It is defined as $Bias(\hat{\theta}) = E[\hat{\theta}] - \theta$. If the Bias is 0, the Estimator is unbiased.

\subsection{Mean Squared Error}

The Mean Squared Error is a measure of how well an Estimator performs. This was also discussed on one of the Tasks handed out. Here we had to Prove that the Mean Squared Error can be decomposed into the Variance of the Estimator and the Bias of the Estimator. 
\begin{align*}
    \text{MSE}(\hatt) &= \text{Bias}(\hatt)^2 + \text{Var}(\hatt) \\
    \Leftrightarrow \E[(\hatt - \theta)^2] &= (\E[\hatt] - \theta)^2 + \E[\hatt^2] - \E[\hatt]^2 \\
    \Leftrightarrow \E[\hatt^2 - 2\hatt\theta + \theta^2] &= \E[\hatt]^2 - 2\E[\hatt]\theta + \theta^2 + \E[\hatt^2] - \E[\hatt]^2 \\
    \Leftrightarrow \E[\hatt^2] - 2\E[\hatt]\theta + \theta^2 &= \E[\hatt^2] - 2\E[\hatt]\theta + \theta^2
\end{align*}


The Mean Squared Error can be decomposed into the Variance of the Estimator and the Bias of the Estimator. If the Mean Squared Error is low, the Estimator is good. The Goal of an Estimator is to minimize the Mean Squared Error.

\subsection{Bias-Variance Tradeoff}
\label{sec:Bias-Variance Tradeoff}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/bias_variance_tradeoff.png}
    \caption{}
    \label{fig:}
\end{figure}

As Depicted in the Figure, the Bias and the Variance are inversely proportional. If the Bias is high, the Variance is low and vice versa. The Goal is to find the sweet spot where the Bias and the Variance are both low. This is called the Bias-Variance Tradeoff.

\subsection{Point Estamation and the Consinstency of the Estimator}
A Point Estimator is consistent if it converges in Probability to the true value of the parameter. This means that the Estimator is getting better and better with more data. Formally this is written as $\lim_{m \rightarrow \infty} P(|\hat{\theta}_m - \theta| > \epsilon) = 0$. 

\subsection{Maximum Likelihood Estimation}

The Maximum Likelihood Estimation is a method to estimate the parameter previously discussed. It is defined as $\hat{\theta}_{ML} = \arg \max_{\theta} p_{model}(X|\theta) = \arg \max_{\theta} \prod_{i = 1}^{m} p_{model}(x_{i}|\theta)$. This means that the Maximum Likelihood Estimation maximizes  $ \hat{\theta}_{ML} $ given the data $X$.
With no poof what so ever we will assume, that the Maximum Likelihood Estimation is consistent, when the true distrubution $p_{data} $ is in the model class and $p_{data}$ is a Value of $\theta$ 

\subsection{(Negative) Log Likelihood}

Since it is numerically Problematic to take the Product, due to overflow/underflow we can take the Logarithm of the Likelihood. This is called the Log Likelihood. It is defined as 

\begin{align*}
    \arg\max_{\theta} \log p_{model}(X|\theta) &= \arg\max_{\theta} \log \prod_{i = 1}^{m} p_{model}(x_{i}|\theta) \\
    &= \arg\max_{\theta} \sum_{i = 1}^{m} \log p_{model}(x_{i}|\theta) \\
    \arg\min_{\theta} \left(- \log p_{model}(X|\theta)\right) &= \arg\min_{\theta} \left(- \sum_{i = 1}^{m} \log p_{model}(x_{i}|\theta)\right)
\end{align*}
The last Equation is the Negative Log Likelihood. It is the same as the Log Likelihood, but with a minus sign in front. This is done to make it a minimization Problem, as most Optimization Algorithms are designed to minimize a Function.

\subsection{Negative Log-Likelihood and Cross Entropy}
Negative Log-Likelihood and Cross Entropy are equivalent when m approaches infinity. This is due to the fact that the Negative Log-Likelihood is the average Cross Entropy. This can be shown as follows:
Given that the samples \( x_i \) are drawn from the true data-generating distribution \( p_{data}(x) \), and the negative log-likelihood (NLL) is divided by the number of samples \( m \), the expression for \( m \to \infty \) approximates the cross-entropy between the modeled distribution \( p_{model}(x|\theta) \) and the true data-generating distribution \( p_{data}(x) \):

\begin{align*}
    \frac{1}{m} \sum_{i=1}^{m} - \log p_{model}(x_i|\theta) &\approx \mathbb{E}_{x \sim p_{data}}[-\log p_{model}(x|\theta)] \\
    &= -\int p_{data}(x) \log p_{model}(x|\theta) \, dx \\
    &\approx H(p_{data}, p_{model})
\end{align*}

As \( m \to \infty \), the average NLL converges to the cross-entropy between \( p_{data} \) and \( p_{model} \).

\subsection{Conditional Negative Log-Likelihood}
We can generalize the Negative Log-Likelihood to the Conditional Negative Log-Likelihood. This is done by conditioning the Negative Log-Likelihood on the input. It is defined as $\arg \min_{\theta} \sum_{i = 1}^{m} - \log p_{model}(y_{i}|x_{i}, \theta)$. This is particularly useful, because it can be used in Classification, Segmentation, among many other Tasks, where we want to Intepret the conditional Probability of the output given the input.

\subsection{Cross Entropy for Classification/Segmentation}
In Classification and Segmentation Problems a Neural Network usually learns Probability Function right away lets call it $p_{\theta}(y|x)$ This leads to the following Negative Log Likelihood: $- \log p(Y|X,\theta) =  \sum_{i} - \log p(y_{i}|x_{i}, \theta)$.  
Thus follows the Cross Entropy Loss: $H = \frac{1}{m} \sum_{i} - \log p_{\theta}(y_{i}|x_{i})$
\subsection{Segmentation and Pixel Wise Classification}
In Segmentation and Pixel Wise Classification the Cross Entropy Loss is calculated for every Pixel. This is done by calculating the Cross Entropy Loss for every Pixel and then averaging over all Pixels. This is done to get a single Value for the Loss.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/pixel_wise.png}
    \caption{}
    \label{fig:}
\end{figure}
The Figure is a visualization of the Cross Entropy Loss for a Pixel Wise Classification Task. Every Layer here represents a Class. So here the average for all Classes is calculated and then compared to get a Classification Result. 

\subsection{Mean Squared Error for Regression}
If we consider the Normal Distribution as the Model Distribution, the mean of that is learned by the Neural Networks, considering most Regression Problems. This leads to the Likelihood being defined as $p(Y|X,\theta, \sigma) = \prod_{i} \mathcal{N}(y_{i},\mu_{\theta}(x_{i}),\sigma)$
Thus follows the Negative Log Likelihood: $- \log p(Y|X,\theta, \sigma) =  \sum_{i} \frac{1}{2} 2(\pi \sigma^{2}) + \frac{1}{2} \frac{(y_{i} - \mu_{i}(x_{i}))^{2}}{\sigma^{2}}$. If we assume that the Variance is constant, we can drop the first term, also the Denominator can be dropped, as it is constant aswell. This leads to the Mean Squared Error: $MSE = \frac{1}{m} \sum_{i} (y_{i} - \mu_{i}(x_{i}))^{2}$
The minimum of the Mean Squared Error is if $\mu(x_{i}$ is the mean of the samples.)


\subsection{Mean Absolute Error}
For the MAE or $L{1}$ a Laplace Distribution is assumed. This leads to the Likelihood being defined as $p(Y|X,\theta, \sigma) = \prod_{i} \mathcal{L}(y_{i},\mu_{\theta}(x_{i}),\sigma)$ With the NLL being defined as $- \log p(Y|X,\theta, \sigma) =  \sum_{i} log(2 \sigma) + \frac{|y_{i} - \mu_{i}(x_{i})|}{\sigma}$. This leads to the Mean Absolute Error: $MAE = \frac{1}{m} \sum_{i} |y_{i} - \mu_{i}(x_{i})|$ The Major difference is the missing squared making it more robust to outliers. But also the minimum is not the mean of the samples, but the median. 

\subsection{Maximum A Posteriori Estimation}
An alternative to the Maximum Likelihood Estimation is the Maximum A Posteriori Estimation. It is defined as $\hat{Theta}_{MAP} = \arg \max _{\theta} p(\theta|X)$
Assuming $\theta$ is a random variable, we can use Bayes Rule to rewrite this as $\hat{\theta}_{MAP} = \arg \max _{\theta} p(X|\theta) p(\theta)$ To further simplify the operations, we can take the log as per usual. This leads to $\hat{\theta}_{MAP} = \arg \max _{\theta} \log p(X|\theta) + \log p(\theta)$ Here we can incoperate a sum again thus follows $\hat{\theta}_{MAP} = \arg \max _{\theta} \sum_{i} \log p(x_{i}|\theta) + \log p(\theta)$
The is as the Name already hints, the Maximum Likelihood Estimation with a Prior. This is useful if we have some prior knowledge about the parameter. With backround knowledge we can push $\theta$ to a realistic value. Say we have knowledge X we can have  $P(Y|X,\theta)$ here the same with x follows $\hat{\theta}_{MAP} = \arg \max _{\theta} \sum_{i} \log p(x_{i}|x_{i}\theta) + \log p(\theta)$
This is a key Concept later on for Regularization of Weights for Neural Networks.

\section{Error functions}
The NLL  $\hat{\theta}_{ML} =  \arg\min_{\theta} (- \sum_{i = 1}^{m} \log p(x_{i}|\theta)) = \arg \min_{\theta} f(\theta)$ the Maximum Likelihood Estimator will now be viewed as the minimum of a Function. One might notice that this does not look too different from minimizing or maximizing a function during Training in Machine-Learning. This would in that Context be reffered to as  Objective Function, cost function, error function or loss function. The Variable $\theta$ that is to be Optimized would then be equivalent to the Parameters of the Model. Thus the Maximum Likelihood Point Estimation is equivalent to a Machine Learning Algorithm. 
The obvious question now is, how do we minimize this function?



\subsection{Local and Global Minima}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/local_global.png}
    \caption{}
    \label{fig:Local vs Global Minima}
\end{figure}
As explained before the Local Minima is the one found by Gradient Descent. This might not be too bad if it is approximitely the same as the Global Minima. But if it is not, the Algorithm will get stuck in the Local Minima. A good rule of thumb is to compare the Local Minima to low values of the cost function, if they are not far off, the Local Minima is good enough.

\newpage
\subsection{Convex Functions}
Convex function if $\forall x,y \in D  \text{and} t \in [0,1] \rightarrow f(tx + (1-t)y) \leq tf(x) + (1-t)f(y)$ hold. This basically just means that every point is below the average of the two points. Convex Functions have the nice property that they have only one minimum. However this is rather rare in the context of Machine Learning.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/convex.png}
    \caption{}
    \label{fig: Illustration of Convexity}
\end{figure}

\subsection{Different approaches to find Minima}

\subsubsection{Least Squares}
Least Squares is a method to find $\hat{\theta}$ for a linear Model this is done by taking $\arg \min_{\theta} ||A \theta - y ||^{2} = (A^{*}A)^{-1} A^{*}y$ This only works for least squares though and Machine Learning Models are usually not linear.

\subsubsection{Grid Search}
Grid Search is a method to find the minimum of a function. It is done by evaluating the function at every point on a grid. This is rather slow, as the number of points grows exponentially with the number of dimensions.

\subsubsection{Evoluationary Algorithms}
Evolutionary Algorithms are a class of algorithms that are inspired by the process of natural selection. They work by creating a population of solutions and then using genetic operators such as mutation and crossover to evolve the population over time. This is rather slow aswell, as it is a brute force method. It works fine for 10-1000 Parameters but becomes inneficient for more. It can be usefull for HyperParameters though. 

\subsubsection{Gradient Based Optimization}
If the Model and the Error Function are differentiable, Gradient Based Optimization can be used. This is done by calculating the Gradient of the Error Function and then following the Gradient to the minimum.

\section{Gradient Descent}
\subsection{Error Surface}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/error_surface.png}
    \caption{}
    \label{fig:Error Surface}
\end{figure}

This is a Geometric Representation of the Errror Function. $w_{A} , w_{B}$ is a local and a Global minimum. Here Gradient Descent comes in handy. If we start at a random point, here $w_{C}$ we can follow the Gradient to the minimum. So basically we just find the derivative of the function, follow the Gradient to the opposite direction and repeat this until we reach the minimum. The opposite direction is taken, as the Gradient points to the steepest ascent, and for minimization we want to go in the opposite direction.

\subsection{Formal Definition of Gradient Descent}
We are looking for $\theta$ that minimizes the Error Function $f(\theta)$ Since we can not do this analytically we will use a numerical method.  For that we choose a starting value $\theta_{0}$ and then update it iteratively. The update rule is defined as $\theta_{i+1} = \theta_{i} + \Delta \theta_{i}$ 
The usual way to do this is to introduce a learning rate $\epsilon$ and then update the parameter as follows $\theta_{i+1} = \theta_{i} - \epsilon \nabla f(\theta_{i})$

\subsection{Gradient - Taking the Derivative in Multiple Dimensions}
The Error Function of a Neural Net is usually dependent on multiple Parameters. This is why it becomes necessary to talk about partial derivatives, thus the concept of Gradients is introduced. Let $f(\theta)$ be a skalar Function. Meaning it mapps from $\mathbb{R}^{n} \rightarrow \mathbb{R}$. The Gradient of $f$ is defined as $\nabla f(\theta)_{i} = \frac{\partial f(\theta)}{\partial \theta_{i}} f(\theta)$ This is just the partial derivative of $f$ with respect to $\theta_{i}$. If u write this in vector form it becomes $\nabla f(\theta) = \begin{pmatrix} \frac{\partial f(\theta)}{\partial \theta_{1}} \\ \frac{\partial f(\theta)}{\partial \theta_{2}} \\ \vdots \\ \frac{\partial f(\theta)}{\partial \theta_{n}} \end{pmatrix}$ So every entry of the Gradient is the partial derivative of $f$ with respect to the corresponding parameter in our cases usually the weight.
A stationary point is given if every entry of the Gradient is 0.

\subsection{Direcional Derivative}
The Directional Derivative describes the rate in which a function changes in a certain direction. It is defined as $\nabla_{u} f(\theta) = \lim_{\alpha \rightarrow 0} \frac{f(\theta + \alpha u) - f(\theta)}{h}$ This is just the derivative of $f$ in the direction of $u$. This can be rewritten as $\nabla_{u} f(\theta) = \nabla f(\theta) u^{T}$ This is just the dot product of the Gradient and the direction.
To minimize f, we must find the direction u, where f decreases the fastest, this means $min_{u,u^{T}u =1} \ u^{T} \nabla f(\theta) = min_{u,u^{T}u =1} ||u||_{2} \cdot ||\nabla f(\theta)||_{2} \cdot \cos(\phi) $.
where $\phi$ is the angle between the Gradient and the direction. Since u is a unit vector, the minimum can be simplified to $min_{u} \cos(\phi)$, since the Gradient is independent of u. This is due to the fact that $\cos$ oxcilates between -1 and 1. Thus the minimun is -1 and $\cos(\pi) = -1$ 

\subsection{Learning Rate $\epsilon$}
For now we will have  just take $\epsilon$ as a small constant. The danger when $\epsilon$ is too large is that may not converge to the minimum. If $\epsilon$ is too small, the convergence will be slow. This is why it is important to choose $\epsilon$ wisely.

\subsection{Jacobi Matrix}
If we now have vectors as inputs and outputs it is handy to define a Matrix called Jacobi-Matrix. This matrix conatins the partial derivative of f at the position x. It is defined as \[ (J_{f}(\theta))_{i,j} = \frac{\partial}{\partial \theta_{j}} f(\theta)_{i} \]
This can be written in the following Matrix Form \[ J_{f}(\theta) = \begin{pmatrix} \frac{\partial f_{1}(\theta)}{\partial \theta_{1}} & \frac{\partial f_{1}(\theta)}{\partial \theta_{2}} & \cdots & \frac{\partial f_{1}(\theta)}{\partial \theta_{n}} \\ \frac{\partial f_{2}(\theta)}{\partial \theta_{1}} & \frac{\partial f_{2}(\theta)}{\partial \theta_{2}} & \cdots & \frac{\partial f_{2}(\theta)}{\partial \theta_{n}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial f_{m}(\theta)}{\partial \theta_{1}} & \frac{\partial f_{m}(\theta)}{\partial \theta_{2}} & \cdots & \frac{\partial f_{m}(\theta)}{\partial \theta_{n}} \end{pmatrix} \]

\subsection{The Derivative of second order}
The derivative of second order is important for us as well since it can imply the improvement that is expected of one iteration of the Gradient Descent. 

\subsection{Hessian Matrix}
The Hessian Matrix is the Matrix of second order partial derivatives of a function. It is defined as \[ H_{f}(\theta)_{i,j} = \frac{\partial^{2}}{\partial \theta_{i} \partial \theta_{j}} f(\theta) \] The directional Derivative of second order to a Directionvector u 
is given by $u^{T} H_{f}(\theta) u$. If u is an Eigenvektor of H, then the derivative of second order is equivalent to the corresponding Eigenvalue. For other directions, the derivative of second order is the weighted average of the Eigenvalues.
Where Directions which correspond better with the Eigenvalues are more important. This also implies that the second directional derivative is beween the smallest and the largest Eigenvalue of H, because it is a weighted sum, it must be contrained by the smallest and the largest Eigenvalue.

This is also equivalent to the Jacobi Matrix of the Gradient. 

\subsection{Approximate Second-Order Methods}
Let's have a look at the Newton Method. The Newton Method is used to find the minimum of a function. It is defined as $\theta_{i+1} = \theta_{i} - H_{f}(\theta_{i})^{-1} \nabla f(\theta_{i})$ This is just the Gradient Descent with the Hessian Matrix. 
I mean it makes sense though, since we basically exchanged our shitty $\epsilon$ to a more dynamic way of choosing the learning rate. 
It brings some disatvantages though, as the Hessian Matrix calculation as well as the inversion can be rather expensive for large disatvantages. 

\section{Stochastic Gradient Descent}
If the calculation of the Gradient is done for the entire Dataset, it might be slow, due to the immense size. Also the data during an iteration is saved in RAM or Graphicsmemory, this can be become impossible to handle for large Datasets. This is why we introduce Stochastic Gradient Descent. Which takes Batches of the Data and calculates the Gradient for these Batches. 
Lets assume that the Trainingsample is i.i.d. then the log likelihood can be written as $L_{\theta}(X) = \frac{1}{n} \sum_{i=1}^{n} L(f_{\theta}(x_{i}),y_{i})$ which implicates that the Gradient can be written as $\nabla_{\theta} L_{\theta}(X) = \frac{1}{n} \sum_{i=1}^{n} \nabla_{\theta} L(f_{\theta}(x_{i}),y_{i})$ This is just the average of the Gradients of the Batches. 

\subsection{Mini-Batch Gradient as an unbiased Estimator}
The Approximation of the Gradient with the help of Mini-Batches can be seen as an unbiased Estimator of the Gradient. In math language we can write 
\begin{align*}
    \mathbb{E} \left[ \nabla_{\theta} L_{\theta} (\hat{X}) \right] 
    &= \mathbb{E} \left[ \frac{1}{k} \sum_{x \in \hat{X}} \nabla_{\theta} L_{\theta} (x) \right] \\
    &= \frac{1}{|\mathcal{X}_k|} \sum_{\hat{X} \in \mathcal{X}_k} \frac{1}{k} \sum_{x \in \hat{X}} \nabla_{\theta} L_{\theta} (x) \\
    &= \frac{1}{k} \sum_{x \in X} \frac{\binom{n-1}{k-1}}{\binom{n}{k}} \nabla_{\theta} L_{\theta} (x) \\
    &= \frac{1}{k} \sum_{x \in X} \frac{k}{n} \nabla_{\theta} L_{\theta} (x) \\
    &= \frac{1}{n} \sum_{x \in X} \nabla_{\theta} L_{\theta} (x) \\
    &= \nabla_{\theta} L_{\theta} (X)
    \end{align*}
    The given equations demonstrate that the expected gradient of the loss function, when computed over a randomly sampled subset of the data, equals the gradient of the loss function over the entire dataset. This result shows that using mini-batches in stochastic gradient descent  provides an unbiased estimate of the full gradient. As a result, SGD can efficiently and effectively optimize the model parameters by using only small subsets of data at each iteration, making it scalable and practical for large datasets.
    (rewritten by chatgpt so it becomes more coherent lol)


\subsection{Batch Size}
The Batch size is as per usual a trade off, either we choose large Batches. This would lead to a more accurate Gradient, but needs to more computation power. If we choose small Batches, the Gradient is less accurate, but the computation is faster. This makes it necessary to choose a smaller learning rate so the training remains stable.
For the Varianz of the Gradient $Var(\frac{1}{|I|} \sum_{i\in I} g_{i}) = \frac{1}{|I|^{2}}\sum_{i\in I}Var(g_{i})= \frac{\sigma^{2}}{|I|}$
I is a set of indices of the Patch and the Gradients are $g_{i}$ This makes sense as it exactly depicts the relation we established before the equation.
The choice of the Batch size, since it is a Tradeoff depends on several Factors
\begin{enumerate}
    \item If the Batches are processed in parallel, the Batch size should be chosen to fit the memory of the GPU.
    \item Multiprozessor Systems can't be used to their full potential if the Batch size is too small. Since the Batches have fixed cost for the computation at a certain size. 
    \item Hardware is optimized for certain Batch sizes. For GPUs this is usually $2^{n}$.
    \item Small Batches have a Regularization effect, as the Gradient is more noisy.
\end{enumerate}

\subsection{Algorithm for Stochastic Gradient Descent}
\begin{algorithm}[h]
    \caption{Stochastic gradient descent (SGD) update at training iteration $k$}
    \begin{algorithmic}
    \Require Learning rate $\epsilon_k$
    \Require Initial parameter $\theta$
    \While{stopping criterion not met}
        \State Sample a minibatch of $m$ examples from the training set $\{ x^{(1)}, \ldots, x^{(m)} \}$ with corresponding targets $y^{(i)}$
        \State Compute gradient estimate: $\hat{g} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} L(f(x^{(i)}; \theta), y^{(i)})$
        \State Apply update: $\theta \leftarrow \theta - \epsilon \hat{g}$
    \EndWhile
    \end{algorithmic}
    \end{algorithm}
    Lets go through the Algorithm step by step. First we give a learning rate and an initial parameter. Then we sample a minibatch of m examples from the training set. We compute the gradient estimate by taking the average of the gradients of the loss function over the minibatch. We then apply the update rule to the parameter by subtracting the learning rate times the gradient estimate. We repeat this process until a stopping criterion is met, which could be a maximum number of iterations, a convergence criterion, or another condition.

    \subsection{Learning Rate $\epsilon$}
    The first Parameter was the Learning Rate $\epsilon$. The Learning Rate needs to go to Infinity, so $\sum_{k=1}^{\infty} \epsilon_{k} = \infty$ and $\sum_{k=1}^{\infty} \epsilon = \infty$ Usually the learning rate is being reduced over time. This is done by a learning rate schedule. For example a linear decay, where the learning rate is reduced by a contant factor until it reaches a certain iteration, where is remains constant. 

    \subsection{Momentum}
    \subsection{Algorithm for Stochastic Gradient Descent with Momentum}
    Momentum is a method to accelerate the convergence of Gradient Descent. Momentum aggregates the Gradients of previous Iterations. $$v \leftarrow \alpha v + \epsilon \nabla_{\theta}(\frac{1}{m} \sum_{i=1}^{m} L(f(\theta^{(i)};\theta),y^{(i)})) $$ This is just the Gradient Descent with the addition of the Momentum. The Momentum is a Hyperparameter and is usually set to 0.9, 0.99, or 0.5. The Momentum is then used to update the parameter as follows $\theta \leftarrow \theta + v$ This is just the Gradient Descent with the addition of the Momentum. The Momentum is a Hyperparameter and is usually set to 0.9, 0.99, or 0.5. The Momentum is then used to update the parameter as follows $\theta \leftarrow \theta - v$
    \begin{algorithm}
\caption{Stochastic gradient descent (SGD) with momentum}
\begin{algorithmic}
\Require Learning rate $\epsilon$, momentum parameter $\alpha$
\Require Initial parameter $\theta$, initial velocity $v$
\While{stopping criterion not met}
    \State Sample a minibatch of $m$ examples from the training set $\{ x^{(1)}, \ldots, x^{(m)} \}$ with corresponding targets $y^{(i)}$
    \State Compute gradient estimate: $g \leftarrow \frac{1}{m} \nabla_\theta \sum_i L(f(x^{(i)}; \theta), y^{(i)})$
    \State Compute velocity update: $v \leftarrow \alpha v - \epsilon g$
    \State Apply update: $\theta \leftarrow \theta + v$
\EndWhile
\end{algorithmic}
\end{algorithm}
\section{Adaptive Learning Rates}
This basically just means that we can adapt the learning rate to the Parameters. There are several methods to do this. 
\begin{enumerate}
    \item AdaGrad scales a learning rate by the inverse of the square root of the sum of all historical squared gradients. This means that the learning rate is reduced for parameters that have received large gradients in the past.
    \item RMSProp is similar to AdaGrad, but uses a moving average of squared gradients instead of the sum of squared gradients. This allows the learning rate to adapt more quickly to changes in the gradients.
    \item Adam combines the ideas of momentum and RMSProp. It uses a moving average of gradients and a moving average of squared gradients to adapt the learning rate for each parameter. 
\end{enumerate}
\subsection{AdaGrad Algorithm}
\begin{algorithm}
    \caption{The AdaGrad algorithm}
    \begin{algorithmic}
    \Require Global learning rate $\epsilon$
    \Require Initial parameter $\theta$
    \Require Small constant $\delta$, perhaps $10^{-7}$, for numerical stability
    \State Initialize gradient accumulation variable $r = 0$
    \While{stopping criterion not met}
        \State Sample a minibatch of $m$ examples from the training set $\{ x^{(1)}, \ldots, x^{(m)} \}$ with corresponding targets $y^{(i)}$
        \State Compute gradient: $g \leftarrow \frac{1}{m} \nabla_\theta \sum_i L(f(x^{(i)}; \theta), y^{(i)})$
        \State Accumulate squared gradient: $r \leftarrow r + g \odot g$
        \State Compute update: $\Delta \theta \leftarrow -\frac{\epsilon}{\delta + \sqrt{r}} \odot g$ \hspace{0.2cm} (Division and square root applied element-wise)
        \State Apply update: $\theta \leftarrow \theta + \Delta \theta$
    \EndWhile
    \end{algorithmic}
    \end{algorithm}
    So the steps added here are the Accumulation of the squared Gradient, and the update. As mentioned in the short description before, since the Multiplikation and the Division are applied element wise, the update is done for every parameter and so every parameter that was previosuly large will have a reduced learning rate. The other algorithms for other adaptive learning rates are similar to this one, but with the steps I descibed before.
    \section{Feed-Forward Networks}

    \subsection{Single-Neuron}

    The output of  single Neuron with D input values $x_{i}$, weights $w_{i}$ and Bias b can be calculated as follows: $$a = \sigma(\sum_{i=1}^{D} w_{i}x_{i} + b)$$ where $\sigma$ is the activation function. The activation function is usually a non-linear function, such as the sigmoid function, the tanh function, or the ReLU function. The output of the neuron is then passed through the activation function to produce a non linear output. This is done, so that the Model can learn non-linear relationships between the input and the output, otherwise the Model would just be a linear regression.

    \subsection{One Layer and multiple Neurons}
    Often a layer is defined by Multiple Neruons, those are fed with the same input and combined in one layer. The output of such a layer can be calculated by $$a = \sigma(\sum_{i=1}^{D}w_{ji} \cdot x_{i} + b_{j})$$ The Compact form of this would be $a = \sigma(Wx + b)$ where $W$ is the Matrix of the weights and $b$ is the Bias.


    \subsection{Multiple Layers}

    Well, now that we have the definition of one Layer down, why not do Multiple and thus create a neural Net. The output of a Neural Net with L layers can be calculated as follows: \begin{align*}
        a^{(1)} &= \sigma(W^{(1)}x + b^{(1)}) \\
        a^{(2)} &= \sigma(W^{(2)}a^{(1)} + b^{(2)}) \\
        &\vdots \\
        a^{(L)} &= f^{out} (W^{(L)}a^{(L-1)} + b^{(L)})
    \end{align*}
    Now we can consider writing this as a parameterized function
  
    \[
\text{nn}_{\theta}(x) = f_{\text{out}} \left( W^{(N)} \cdot f^{(N-1)} \left( \ldots W^{(2)} \cdot f^{(1)} \left( W^{(1)} \cdot x + b^{(1)} \right) + b^{(2)} \ldots \right) + b^{(N)} \right)
\]

With the parameters:

\[
\theta = \left( (W^{(1)}, b^{(1)}), (W^{(2)}, b^{(2)}), \ldots, (W^{(N)}, b^{(N)}) \right)
\]
The f I use is just a placeholder for the activation function.

How many layers exist is something we will define with the amount of layers being the amount of weight matrices used for Multiplication. 

\subsection{Fun Facts for Linear Activation}
Lets say we decide to use a linear function, lets show by its properties that it is not a good choice. 

\[
\text{nn}_{\theta}(x) = f_{\text{out}}\left(W^{(N)} \cdot f^{(N-1)}\left(\cdots f^{(2)}\left(W^{(2)} \cdot f^{(1)}\left(W^{(1)} \cdot x\right)\right)\right)\right)
\]

This can be simplified by grouping the weight matrices and activation functions:

\[
\text{nn}_{\theta}(x) = f_{\text{out}}\left(W^{(N)} \cdot f^{(N-1)}\left(\cdots f^{(2)}\left(W^{(2)} \cdot W^{(1)} \cdot f^{(1)}(x)\right)\right)\right)
\]

Combining weights into a single matrix \(W'\):

\[
\text{nn}_{\theta}(x) = f_{\text{out}}\left(W^{(N)} \cdots W^{(2)} \cdot W^{(1)} \cdot f^{(N-1)} \circ f^{(2)} \circ f^{(1)}(x)\right)
\]

Finally, we get:

\[
\text{nn}_{\theta}(x) = f_{\text{out}}\left(W' g(x)\right)
\]

\subsection{Activation Functions}
The Activation Function is a non-linear function that is applied to the output of a neuron. As shown before we do need those non-linear Activation Functions to learn non-linear relationships. We can differentiate beween two Sorts of here, one being for Hidden Layers (So the Layers between the Input and the Output) 
and the other for the Output Layer. Some of the Hidden Layer Activation Functions are:
\begin{itemize}
    \item Sigmoid: $\sigma(x) = \frac{1}{1 + e^{-x}}$
    \item Tanh: $\tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$
    \item ReLU: $ReLU(x) = \max(0,x)$
    \item Hard Tanh: $HardTanh(x) = \max(-1, \min(1,x))$
    \item Softplus: $Softplus(x) = \log(1 + e^{x})$
    \item 
\end{itemize}
Some of the Output Layer Activation Functions are:
\begin{itemize}
    \item Linear: $f(x) = x$
    \item Sigmoid: $\sigma(x) = \frac{1}{1 + e^{-x}}$
    \item Softmax: $Softmax(x) = \frac{e^{x_{i}}}{\sum_{j} e^{x_{j}}}$
\end{itemize}
For the output Functions it is maybe important to note that you can use the Sigmoid Function for Binary Classification and the Softmax Function for Multiclass Classification. 

\subsection{Symmetries in Parameter Space}
There are many ways to achieve the same mapping of input to output values, even though the weight matrices and biases are different. One way is to permute the neurons in a layer, this leads to $M!$ equivalent solutions. Some activation functions are also point reflections, meaning that $f(x) = f(-x)$, this leads to $2^{M}$ equivalent solutions. 
An Example for this would be $tanh(-x) = -tanh(x)$ If we combine those symmetries, we get $2^{M} \cdot M!$ equivalent solutions. This also there are as many equivalent local Minima. Which is ok though, as large networks local Minimas are usually quite good. 

\subsection{Universal Approximation Theorem}
This Theorem  states that if we have 2 layers, with enough hidden units, 
we can approximate any Continuous Function.

\section{Backpropagation}
\subsection{Why do we need Backpropagation?}
 As discussed in Chapter 5 we use gradient descent to minimize the error function. The error function is a function of the parameters of the model, and we need to compute the gradient of the error function with respect to the parameters in order to update them. 
 Perhaps a rather simple approach would be to to calculate the Difference quotient being defined by $$\frac{\partial L}{\partial W_{ji}} = \frac{L(W_{ji} + \epsilon) - L(W_{ji} - \epsilon)}{2 \epsilon} + \underbrace{O(\epsilon^{2})}_{residual \ corrections}$$ This is a rather simple approach, but it is not very efficient, since you need to evaluate the Loss-Function twice for every Parameter. As this is why we use the Backpropagation Algorithm.

 \subsection{Lets look at the Chain Rule}
 For Backpropagation we first need to understand the chain rule. The chain rule in 1D is defined as $(g \circ f)' (x) = g' (f(x)) \cdot f'(x)$. This is also written as $\frac{x}{\partial x} g(f(x)) = \frac{\partial g}{\partial f} \frac{\partial f}{\partial x}$.
 If our function depends on multiple variables, for example $x$ and $y$ then we can look at the partial derivatives. Meaning the partial derivative of $f(x,y) $ with respect to $x$ can be written as $\frac{\partial f}{\partial x}$. And with respect to $y$ as $\frac{\partial f}{\partial y}$. 

 \newpage
 \subsection{Direcional Derivative with the Chain Rule}
 \begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/richtungsableitung.png}
    \caption{}
    \label{fig:Richtungsableitung}
\end{figure}
Ok lets say our goal is to calculate the partial derivative of f to s, on the left graph.  Well to analyze we take the chain rule, meaning we can express $f$ as $f(x(s),y(s))$ and can then write the partial derivative of $f$ with respect to $s$ so $\frac{\partial f}{\partial s} = \frac{\partial f}{x} \frac{\partial x}{\partial s} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial s}$. This is just the chain rule applied to the partial derivatives.
So if more Dimensions follow, we can use the same pattern. 
\begin{center}
    

\begin{tikzpicture}
    % Nodes
    \node (s) at (0,0) {$s$};
    \node (x) at (3,2) {$x(s)$};
    \node (y) at (3,0) {$y(s)$};
    \node (z) at (3,-2) {$z(s)$};
    \node (f) at (6,0) {$f(x,y,z)$};
    
    % Arrows
    \draw[->] (s) -- (x) node[midway, above, sloped] {$\frac{\partial x}{\partial s}$};
    \draw[->] (s) -- (y) node[midway, above] {$\frac{\partial y}{\partial s}$};
    \draw[->] (s) -- (z) node[midway, above, sloped] {$\frac{\partial z}{\partial s}$};
    \draw[->] (x) -- (f) node[midway, above, sloped] {$\frac{\partial f}{\partial x}$};
    \draw[->] (z) -- (f) node[midway, above, sloped] {$\frac{\partial f}{\partial z}$};
    
    % Equation
    \node at (0, -4) {
    \begin{minipage}{\textwidth}
    \begin{equation*}
        \frac{\partial f}{\partial s} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial s} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial s} + \frac{\partial f}{\partial z} \frac{\partial z}{\partial s}
    \end{equation*}
    \end{minipage}
    };
    
\end{tikzpicture}
\end{center}
This concept can easily be expanded to more functions, lets say we also have $g$ we then need to calculate $g$ in respect to $s$, to do this we can say $$\frac{g}{\partial s} = \frac{\partial g}{\partial f} \frac{\partial f}{\partial s}$$  Since I had trouble understanding it, lets make a quick example for $g$ and $f$. 
If g and f are dependent on one variable s. 
\[
f(s) = s^2
\]
\[
g(f) = f + 1
\]

We want to find \(\frac{\partial g}{\partial s}\).


1. Compute \(\frac{\partial f}{\partial s}\):
\[
\frac{\partial f}{\partial s} = \frac{\partial (s^2)}{\partial s} = 2s
\]

2. Compute \(\frac{\partial g}{\partial f}\):
\[
\frac{\partial g}{\partial f} = \frac{\partial (f + 1)}{\partial f} = 1
\]

3. Apply the chain rule:
\[
\frac{\partial g}{\partial s} = \frac{\partial g}{\partial f} \cdot \frac{\partial f}{\partial s} = 1 \cdot 2s = 2s
\]

Thus, the derivative of \(g\) with respect to \(s\) is:
\[
\frac{\partial g}{\partial s} = 2s
\]
With more variables and more functions this becomes increasingly annoying to write down, which is why it looks so complicated in the Slides. But with Neural Nets we typically have many layers, thus making these "graphs" unsuitable for the task.
Now this where Backpropagation finally comes in handy.
\subsection{Backpropagation Algorithm Intuitively}
Well lets talk about Backpropagation in a more intuitiv manner before discussing the calculus involved. We first run the forward pass so we pass in Training examples and then calculate the output of the Neural Net. If we think of classifiying a number for example, this will most likely not be the correct output. So we calculate the error of that output thus giving us the loss, so how far off our predictions are from the actual values.
Ok now we can apply the backward pass. We basically go back through the layers and check where mistakes were made. Thus we calculate how much each weight in the network contributed to the error. So now we can adjust the weights to minimize the loss in the Neural Net. We can do this over and over again for each Training example, until the loss is minimized. Chapter 3 of the 3B1B Video Series on Neural Networks might be worth to check out. 


\subsection{Backpropagation Algorithm Formally}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/3b1b_backpropagation.png}
    \caption{3B1B Backpropagation}
    \label{fig:Backpropagation}
\end{figure}

Ok lets first look at simple example and then expaldn it, lets say we only have 1 Neuron per layer now lets introduce some notations, as shown in the image. Lets focus on the connection of the last 2 Neurons first. $a^{L}$ being the activation of the last neuron, and $a^{(L-1)}$ being the activation of the previous neuron. So how do we determine the last activation well as we discussed it is determined by a weight $w^{(L)}$ the previous action $a^{(L-1)}$ and a bias $a=b^{(L)}$ and then an activation function. We also dicussed that we define a name for that here being $z^{(L)} = w^{(L)} \cdot a^{(L-1)} + b^{(L)}$ and $a^{(L)} = \sigma(z^{(L)})$. 
Now that we computed a as shown in the image we can compute the cost with a target value $y$ and a cost function $C$ This can be expanded to substituting the $a^{L-1}$ function as well of course. But lets focus on the last layer for now. The first thing that might be interesting here is how changes in the weight effect our cost function. In mathematical Terms we are 
interested in the derivative of the cost function with respect to the weight. $ \frac{\partial C_{0}}{\partial W^{(L)}}$ But how do we get there again with intuition the chain rule makes perfect sense here because what is important for that ratio to know. Well first $w^{(L)}$ effects $z^{(L)}$ which in turn effects $a^{(L)}$ which in turn effects the cost function. 
So lets write this down mathematically using the chain rule! 

\[
\frac{\partial C_{0}}{\partial W^{(L)}} = \underbrace{\frac{\partial z^{(L)}}{\partial w^{(L)}}}_{\shortstack{effect of \( w \) \\ with respect to \( z \)}} \underbrace{\frac{\partial a^{(L)}}{\partial z^{(L)}}}_{\shortstack{now that \( z \) changed \\ let's calculate the effect \\ that has on \( a \)}} \underbrace{\frac{\partial C_{0}}{\partial a^{(L)}}}_{\shortstack{effect of \( a \) \\ on the cost \( C_{0} \)}}
\]
Lets break down the terms now. So first the effect of a on the term C is just $2(a^{(L)} - y)$ The derivative of the activation function is just the derivative of the activation function. And the derivative of $z$ with respect to $w$ is just $a^{(L-1)}$. Because this relation just depends on ow strong the previous neuron was activated. Now this is just for a single training example if we wanted to compute this for 
all the Training examples we would compute an average meaning $$\frac{\partial C}{\partial W^{(L)}} = \frac{1}{n} \sum_{k = 0}^{n-1} \frac{\partial C_{k}}{\partial W^{(L)}}$$ Now we have succesfully computed one component of the gradient vector $C$ We would now need to do the same for the bias and the previous layers. For the bias the computation 
is almost the same, because we can just swap out every w for a b. But if we consider the derivative of $\frac{\partial z^{(L)}}{\partial b^{(L)}}$ we see that this is just 1. Because obviosuly a change in the Bias would result in a change of the activation exactly equivalent to the change in the Bias. Ok now lets look at how we go backward. The sensitivity of the cost function with respect to the activation of the last layer. In the formula it would look like this: \[ \frac{\partial C_{0}}{\partial a^{(L-1)}} = \frac{\partial z^{(L)}}{\partial a^{(L-1)}} \dots \] which in turn would just be the weight. Now since we kept track 
of this we can trace back how sensitive the cost function is to the activation of the previous layer. This is the basic idea of Backpropagation. Now this can be expanded to multiple neurons, however the Idea stays the same. One thing that changes is the cost function with respect to the activation of the previous layer. Lets introduce a new indeci to keep track which neuron talking about lets call it activation of the kth neuron in the L-1 layer with $a_k^{(L-1)}$ 
thus the cost function with respect to the activation of the kth neuron in the L-1 layer would be $\frac{\partial C_{0}}{\partial a_k^{(L-1)}}$ This can be computed by the chain rule as well, but we need to consider the entire layer L so we compute the sum which leads to the following equation $$ \frac{\partial C_{0}}{\partial a_k^{(L-1)}} = \sum_{j=0}^{n_{L}-1} \frac{\partial z_{j}^{(L)}}{\partial a_k^{(L-1)}} \frac{\partial a_j^{(L)}}{\partial z_j^{(L)}} \frac{\partial C_{0}}{\partial a_j^{(L)}}$$ 
I personally found the slides to be rather confusing that is why i mainly used 3B1B 4th video of his Neural Network Series to explain this.
\subsection{TODO explain this with jacobi matrices (i think before should work to understand the concept though)}

\section{Regularization}

\subsection{Gradient Clipping}

Sometimes Deep Neural Networks, have the  issue of exploding Gradients. This Problem occurs if Multiplying large weights. Visually this is explodingly steep parts in a Loss Function. The Problem is that with our current approach the Parameters of the Model would be updated by a large amount, making our previous progress in Optimizing useless. 
This is why we introduce Gradient Clipping. This is done by setting a Threshold for the Gradient. Here 2 Methods are common, the first one is to clip the Gradient if an Entry of the Gradient exeeds a certain Threshold, this is refered to as Value-Clipping. The second one would be Norm-Clipping if the Gradient exceeds a 
certain Norm, the Gradient is scaled down to the Threshold, this is refered to as Norm-Clipping.

\subsection{Parameter Initialization}

\subsubsection{Very Small Weights}
Lets talk about what happens with certain weights, starting with small weights. If the weights are too small, the Network might collaps to a linear Modell, which as previosuly discussed is not what we want. If we use the Sigmoid Activation Function and apply it to $W \cdot x$ with W being very small the Function becomes linear.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/sigmoid_small_weights.png}
    \label{fig:Sigmod Small weights}
\end{figure}
\newpage
\subsubsection{Very Large Weights}
Problems also arise with Very Large weights. With very large weights the Sigmoid Function becomes very steep, which makes the Gradient very small. This is a problem because the Gradient Descent would be very slow.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/sigmoid_large_weights.png}
    \caption{Sigmod with Large Weights}
    \label{fig:Sigmod Large weights}
\end{figure}

\subsection{Scalng the Input Values}
The range of Inputs determines the range of values for the Weights, thus making it important to scale the Input Values, because as we have seen this determines the quality of the Neural Net. A good rule of thumb is to scale the Inputs to have a mean of 0 and a standard deviation of 1. 
\subsection{Weight Initialization}
Usually the weights are initialized randomly with small values. So the network starts linear, but this will not remain the case when the weights are updated during training. Some might think it would make sense to initialize the weights with 0, but this would be problematic with the simplest reason being that 
the input for second layer would be the same for all Nodes, the output would be the same as well and would then be multiplied by the same weights. This would lead to the same output for all Nodes in the second layer, this goes on until the output layer, where it will still be zero.  Another reason would be that backpropagation would not
work, as the weights will be multiplied by a value determined by backpropagation but since the weights are zero they will remain zero, making our whole progress useless. 

\subsection{Parameter Initialization}
We want to initialize our parameter so that the Eigenvalues of the Jacobi Matrix and the Weights are close to 1. If the Eigenvalues are larger than 1, the Gradient will explode, if they are smaller than 1, the Gradient will vanish. This means that for the Jacobi Matrix of the actiavtion function with $J_{f^{(i)}}(a^{(i)})$ we want 
the derivative of the activation function to be close to 1. This is different from activation activation function to activation function though.  \\ 
\begin{itemize}
    \item Input Signals are usually scaled to have a mean of 0 and a standard deviation of 1. 
    \item Since the standard deviation of the output of a neuron is  1, the weights should be initialized and multiplied $\frac{1}{\sqrt{n}}$ with n being the number of inputs. This ensures ensures that the variance of the weighted sum remains 1, because the deviation of the weights is $\sigma^{2}$.
    \item The bias changes the mean of the output, usually it remains $0$ sometimes it makes sense to initialize it differently however. An example for that would be ReLU, where the bias should be initialized with a small positive value, so values lower than 0 are passed through the ReLU function.
\end{itemize}
\subsection{Scaling of the Weights as an Hyperparameter}
If we have enough computational power, we can use the scaling of the weights as a hyperparameter. This means that we can try different values for the scaling of the weights and see which one works best we can just experiment on Mini Batches though and check wether the Hidden Units are getting lower each layer. 

\subsection{Overfitting and Underfitting quick comeback in Context}
While Input and Output are usually fixed, the number of Hidden Units is a Capacity Parameter, we need to figure out. If we use too many neurons, the network will overfit, if we use too few neurons, the network will underfit. Deep Neural Networks are usually volotile to Overfitting, due to its high capacity. To prevent that Regularization 
is used.
\subsection{Definition of Regularization}
Regularization can be defined as any modification of a learning algorithm with the purpose of reducing the generalization error without significantly increasing the training error.
Common strategies to achieve this are adding more Terms to the Loss and Extra Constrains to the Modelparameters. Those strategies can use prior knowledge about the problem, but can also try to find a simple model for good generalization.
\subsection{Regularization in Deep Learning}
Most Regularization strategies want to find a good Bias Varianz Tradeoff. Which we have also talked about at the beginning. 

\subsection{Parameter Norm Penalties}
One way too force our Model to have a certain complexity is a so called Penality Term. This Term is added to the Loss Function. The Term is controlled by a Hyperparameter $\alpha$ and the Norm of the Parameters.   
\[
\tilde{L}(\theta; X, y) = \underbrace{L(\theta; X, y)}_{\text{Original Loss Function}} + \underbrace{\alpha \cdot \Omega(\theta)}_{\text{Penalty Term}}
\]

The Parameter Norm Penalty usually only applies to large Values in the weights, but not in the Biases, since the Bias only Influences a single Variable, meaning it needs little Data to be learned. If we would Introduce a Penalty Term for the Bias, underfitting becomes our Problem. 

\subsection{L2 Regularization}
A common and simple approach is the L2 Regularization. The L2 Regularization is defined as the sum of the squares of the weights. 
\[
\tilde{L}(\theta; X, y) = \underbrace{L(\theta; X, y)}_{\text{Original Loss Function}} + \underbrace{\alpha \frac{1}{2} ||w||^{2} }_{\text{L2 Norm Penalty Term}}
\]
This is also known as weight decay, because the weights are decaying to zero. In the Context of Backpropagation this means that the Gradient of the Penalty Term is added to the Gradient of the Loss Function. 
\subsubsection{TODO There is an intuition slide in the slides, which i cant explain well}
\subsection{L1 Regularization}
We have had the L2 Regularization, now lets talk about the L1 Regularization. The L1 Regularization is defined as the sum of the absolute values of the weights. For the loos function this means
The loss Function now is defined as:
\[
\tilde{L}(w; X, y) = L(w; X, y) + \alpha \cdot \|w\|_1
\]

and the Gradient:
\[
\nabla_w \tilde{L}(w; X, y) = \nabla_w L(w; X, y) + \alpha \cdot \text{sign}(w)
\]

\subsection{Sparse Networks}
The advantage of L1 Regularization is that it leads to sparse Networks. This means that the Network has many weights that are zero, so not Unit every unit in one layer is connected to every unit in another layer. This is useful because it saves computation time. 
\newpage
\subsection{Representation Sparsity}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{images/sparsity.png}
    \caption{Different Sparsity}
    \label{fig:Sparsity}
\end{figure}
As visible in the figure in the Second way of writing the Vector the Vector multiply just forces Values to be sparse, since the Vector has Zero Values, while the weight Sparsity sets values of the Martix to $0$. 

\subsection{Regularization as Maximum A Posteriori Estimation}
The L1 and L2 Regularization can also be interpreted as Maximum A Posteriori Estimation. This means that we are looking for the most probable parameters given the data. The L2 Regularization can be interpreted as a Gaussian Prior, while the L1 Regularization can be interpreted as a Laplace Prior. This is not carried out in the slides, but I guess if you leave out the 
constant erm of the gaus prior we see that $\alpha$ is equivalent to $\frac{1}{2 \sigma^{2}}$. This surely works pretty similar for the Laplace distrubution.

\section{Reduction of Modelcapacity}
\subsection{Early Stopping}
One way to reduce the Modelcapacity is to stop the training early. This is done by monitoring the Validation Set and stopping the training when the Validation Error starts to increase. This is done because the Model is starting to overfit. The problem is that the Model might overfit faster, due to it recognizing patterns fast, it shouldnt have by training with the Validation Set.

\subsection{Exponential Moving Average}
We have learned about SGD already, here and issue might be the noise from the random selection of training data, this could lead to instable updating, therefore we introduce EMA which is defined as $$\theta_{EMA,t+1} = (1-\lambda) \theta_{EMA,t} + \lambda \theta_{t}$$ If $\lambda = 1$ then the EMA is just the parameter itself. If $\lambda = 0$ then the EMA is just the previous EMA. 

\subsection{Parameter Sharing}
In Regularization we assumed that we know, what the Model should look like, and thus can know which value will make sense and nudge the model in that direction, for example 0. This is not always the case, but sometimes we have knowldege the knowledge that for example two Models are close together. 
So lets say we have a Model A and a Model B with their respective parameters, we could apply a Parameter Penality, to force the Models to be similar, since we know they are. This would look like this $$ \Omega (\theta_{A}, \theta_{B}) = ||\theta_{A} - \theta_{B}||^{2}_{2} $$ 

We could also force the Parameters to be equivalent, which is done by Parameter Sharing. By Sharing parameters we essenstially tie weights together, meaning updates to the weights from one part of the Model will directly effect the others. If we think in text of Convolutional Neural Networks, we can think of the same filter being applied to different parts of the image. (This will be discussed later)

\subsection{Data Augmentation}
Data Augmentation is a technique to increase the size of the training set. This is done by applying transformations to the training data. (The one Python task from the practical Worksheet with the CIFAR Dataset)
\subsection{Injecting Noise}
Well we use Data Augmentation to make the Data a bit noisy so that the Neural Net is able to deal with Noise. We can also add noise to hidden units. 
We can even add noise to the weights. 
\subsection{Noisy Outputs}
We can make the Outputs Noisy as well to make the Model robust to the Training Label being incorrect. This is done by saying our training label is just correct with a certain probability $1 - \epsilon$ 

\section{Bagging and Dropout}
\subsection{Bagging}
Bagging comes from the term Bootstrap Aggregating. This is basically the Introduction of Democracy. We train several models separately and then average the results, so every Model has the same vote in the final decision. This can be written as $$ p_{bagging} = \frac{1}{k} \sum_{i=1}^{k} p_{i}(y|x)$$ 

\subsection{Expected Error of Bagging}
If the test samples all do the same error, the expected error of the bagging is the same as the expected error of the single model. If the test samples do different errors, the expected error of the bagging is smaller than the expected error of the single model. 

\subsection{Creating the Models for Bagging}
The Models for Bagging are created by training the Model on different subsets of the training data. This is done by sampling the training data with replacement. This means that the same sample can be selected multiple times. This quite computationaly expensive, therefore we take a closer look at Dropout.
\subsection{Dropout}
Dropout is also to reduce complexity and overfitting, but it is an efficient way of performing Bagging, it just omits data during training. This is done by setting the output of a neuron to 0 with a certain probability. This is done for every neuron in the network. It just trains ensembles of all possible sub networks.(An ensemble refers to a combination of multiple models to solve a particular problem.) TODO why is this more efficient?

\subsection{Inference in Dropout}
First talk about how we calculate the Dropout. The Dropout $p_dropout$ is calculated by summing over all binary masks $\mu$ averaging over all possible sub networks, which can be formed by applying different binary masks. Now this is not computationally feasible, so we approximate this by randomly sampling binary masks, which turns out to be quite a good approximation. 
\subsubsection{Weight Scaling Inference Rule}
Ok so during Dropout we set the Neurons to zero with a certain Probabiliy $p$, as discussed before. But now we would like to use the entire Network, so we use something called Weight Scaling. During Inference we scale the Network Weights to $1-p$, this scaling accounts for the fact that during training we set the Neurons to zero with a certain probability.

\subsection{Bagging vs Dropout}
One major difference is that Bagging trains every model independently, while dropouts Subnetworks share Parameters, thus making it possible to compute even regarding the exponential number of possible Models. In Bagging every single Model is trained until converging this is not the case for Dropout. Due to the sheer Size of Dropout, we just train a fraction of the 
possible Models, then sharing is caring comes into play and we share the Parameters across the Models. All differences asside both methods are quite similar as Dropout Subnetworks can also be seen as Subsets of the original Network, which also are created when performing Bagging.

\subsection{Effectivity of Dropout}
Dropout is very effective in Regularization, and can be combined with other Regularization Techniques, we were Previosuly discussed. Dropout computes a Binary Mask per Training Sample in $O(n)$, and needs $O(n)$ Memory, to store the Binary Mask for Backpropagation. Weight Scaling is applied once for all Modelweights, thus making it Computationally irrelevant for Dropout. 
For Dropout it is important to note that we need large Models, since we reduce the Capacity of the Network by Dropout.

\subsection{Dropout in the Context of Noise Augmentation}
We used and learned about Augmentation, some might have noticed that Dropout adds Noise to the Network, as we loose out on some Information, and the Network is forced to learn with missing Information. 

\section{Basics of Convolutions}
NOTE: The following Introcution to Groups is brought to you by chatgpt and the slides as i am too lazy to Tex this. \\\\
Let \( G \) be a set and \( \cdot : G \times G \rightarrow G \) a binary operation. Then \((G, \cdot)\) is a group if:
\begin{enumerate}
    \item There exists an identity element \( e \in G \) such that \(\forall a \in G: e \cdot a = a \cdot e = a \).
    \item Every element \( a \in G \) has an inverse element \( a^{-1} \in G \) such that \( a \cdot a^{-1} = a^{-1} \cdot a = e \).
    \item Associativity holds: \(\forall a, b, c \in G : (a \cdot b) \cdot c = a \cdot (b \cdot c)\).
\end{enumerate}

\section*{Group Operation}

Let \((G, \cdot)\) be a group and \( X \) a set. Then \( \triangleright : G \times X \rightarrow X \) is a (left) group operation if:
\begin{enumerate}
    \item Identity: \(\forall x \in X : e \triangleright x = x\), where \( e \) is the identity element of \( G \).
    \item Compatibility: \(\forall a, b \in G, x \in X : (a \cdot b) \triangleright x = a \triangleright (b \triangleright x)\).
\end{enumerate}

\section*{Examples: Group Operations}

Transformations can be elegantly described using group operations. Let \( X = [0, 1]^{n \times n} \) be the space of images of size \( n \times n \).

\begin{enumerate}
    \item \( G = \{ \text{Id}, \text{Ref}_y \} \) forms a group containing the identity element (Id) and a reflection over the y-axis (Ref\(_y\)). That is, \(\text{Id} \triangleright x = x\) and \(\text{Ref}_y \triangleright x\) corresponds to the mirrored image of \( x \).
    \item \( G = \{ \text{Id}, \text{Rot}_{90}, \text{Rot}_{180}, \text{Rot}_{270} \} \) forms a group containing all 90 rotations of the image. What are the inverse elements of \(\text{Rot}_{90}\), \(\text{Rot}_{180}\), and \(\text{Rot}_{270}\)?
    \item \( G = \{ \text{Rot}_{\theta} : \theta \in \mathbb{R} \} \) forms a group containing all rotations. What problems could arise for a discrete and \( n \)-limited image space?
    \item \( G = \{ T(x, y) : x, y \in \mathbb{R} \} \) forms a group containing all translations by \( x, y \). What problems could arise for a discrete and \( n \)-limited image space?
    \item \( G = S(M) \) is the group of all permutations of a set \( M \). This group plays an important role, for example, in point cloud data.
\end{enumerate}

\section*{Equivariance and Invariance}

Let \( f : X \rightarrow Y \) be a mapping (for example, a neural network), \((G, \cdot)\) a group, and \(\triangleright : G \times X \rightarrow X\) and \(\square : G \times Y \rightarrow Y\) group operations on \( X \) and \( Y \) respectively.

\( f \) is called equivariant with respect to \( G \) if:
\[
\forall g \in G, x \in X : f(g \triangleright x) = g \square f(x)
\]

\( f \) is called invariant with respect to \( G \) if:
\[
\forall g \in G, x \in X : f(g \triangleright x) = f(x)
\]

\subsection{Convolution/Cross Correlation}
Ok now after the gpt Introduction to Groups, lets talk about Convolutions and Cross Correlations. 

For this I recommend the 3B1B Video on Convolutions, but lets try to explain it here as well. To quote the Video "The formulaic definition ... can look quite Intemidating".

But lets start slow and easy, imagine we have two lists of numbers (1,2,3) (4,5,6) and we can want to calculate the Convolution here. We would flip around list 2. So we would have (6,5,4) and then we would slide it over list 1. We would then multiply the corresponding elements and sum them up. So we have $(4 \cdot 1, 1 \cdot 5 + 2 \cdot 4, 1 \cdot 6 + 2 \cdot 5 + 3 \cdot 4, 
2 \cdot 6 + 3 \cdot 5, 3 \cdot 6) = (4, 13, 28, 27, 18)$. This is the Convolution of the two lists. Ok now lets look at the Formula.
The convolution of a function \( x(t) \) with a function \( y(t) \) is defined as:

\[
(x * y)(t) = \int_{-\infty}^{\infty} x(\tau) y(t - \tau) \, d\tau = \int_{-\infty}^{\infty} x(t - \tau) y(\tau) \, d\tau
\]
 
Now the $\tau $ is basicallty the flip to our list 2, just to give some intuition to the integral, considering our example the last equivalence in the definition makes sense as well, we could also reverse the first list and slide it over the second list, to achieve the same result. Here we can introduce the cross correlation, 
which is defined as: 
\[
(x \star y)(t) = \int_{-\infty}^{\infty} x(\tau) y(t + \tau) \, d\tau = \int_{-\infty}^{\infty} x(\tau  -t) y(\tau) \, d\tau = (x(- \tau ) * y(\tau))(t)
\]
This would be like multiplying the first list with the second list, but not flipping the second list, which would be the same as flipping the first list and sliding it over the second list. 
Ok lets visualize this with an example from the Slides. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/convolution.png}
    \caption{Convolution}
    \label{fig:Convolution and Cross Correlation}
\end{figure}
Ok lets break down Convolution first, lets take the function y and flip like we have done to the list, and then we slide it over function x. Except the script image is explaining it with the second equivalence thus we flip x and slide it over y. The peak of the new function is basically in our example of the lists, where both lists are multiplied and added together. So imagine the first function coming closer to y and then we start multiplying
the corresponding components, so the value begins to increase and reaches its peak when both function overlap completly and then it decreases again. Lets have a closer look at the Slope of the Convolution here. The Slope is rather steep at the start and then decreases again, this is because the start of function y is steeper (imagine a larger number in the list)
decreasing it is not as steep, since the numbers are smalle for y. Now lets have a look at cross correlation, we just slide x over y, so the slope is not as steep at the start because the smaller part of x slides over y first. To put all of this into the two lists context, we can imagine the list y being (6,5,4) so the Bump at the start is our largest value at the start, try to think about this and watch the 3B1B Video. 
This example makes everyhting more intuitiv for me personally, how ever after reading this I do see if the explanation is a bit messy and all over the place, sorry for that.

\subsection*{Important Properties of  and Cross Correlation}

\begin{itemize}
    \item \textbf{Commutative:} \( x * y = y * x \)
    \item \textbf{Associative:} \( x * (y * z) = (x * y) * z \)
    \item \textbf{Distributive:} \( x * (y + z) = x * y + x * z \)
    \item \textbf{Convolution Theorem:} \( \mathcal{F}(x * y) = (2\pi)^{n/2} \mathcal{F}(x) \cdot \mathcal{F}(y) \), where
    \[
    \mathcal{F}(y)(f) = (2\pi)^{-n/2} \int_{\mathbb{R}^n} y(t) e^{-i f t} \, dt
    \]
    is the Fourier transform.
    \item Equivariant with respect to translations of \( x \) and \( y \).
\end{itemize}

\subsection*{Important Properties of Cross-Correlation}

\begin{itemize}
    \item \textbf{Distributive:} \( x \star (y + z) = x \star y + x \star z \)
    \item Equivariant with respect to translations of \( y \).
\end{itemize}
\subsection{Discrete Convolution/Cross Correlation}
With all my brambling about the lists, discrete Convolution should be rather intuitiv, we just sum up the products of the corresponding elements. The Discrete Convolution is defined as:
\[
(x * y)[n] = \sum_{k \in \mathbb{Z}^{n}} x[k] y[n - k] = \sum_{k \in \mathbb{Z}^{n}} x[n - k] y[k]  
\]
The Discrete Cross Correlation is defined as:
\[
(x \star y)[n] = \sum_{k \in \mathbb{Z}^{n}} x[k] y[n + k] = \sum_{k \in \mathbb{Z}^{n}} x[k-n] y[k]
\]

Here if you go over the list and stay within the bounds of the list that k provides, you are even good to go to see this as the formula from the first example. 

Ok but all of this kind of begs the question of why, why would we do that? In Visual Computing we often have images we refer here as a signal which is just a matrix of Pixel values(Probably a vector containing RGB or something), and we then want to apply a filter to the image this filter, which would be our function y is called a kernel. The Kernel is just a Small Matrix, which we slide over the image(yes like the 2 list of numbers) 
and then we apply the Convolution. (To get great visuals on this, i recommend the 3B1B Video on Convolutions). 
\newpage
\section{Convolutional Layers}
\subsection{Convolutional Layers in 1D}
We are going to use cross correlation now, lets say we have an Input Signal y, with the Kernel x(which is as mentioned before just a matrix) we are going to calculate the cross correlation $s = x \star y$. After calculating the cross correlation we apply an Activation Function to the result. This is the basic idea of a Convolutional Layer. It is also important to mention that we use weight sharing 
meaning we apply the same Kernels to every for example part of the image. This reduces the amount of Parameters when compared to fully connected layers. Also the connection are less, because we only apply our Kernels to a small part of the image.  This allows for the Sparsity discussed previously in connection to L1 Regularization.  An example of this can be seen in the following figure from the Slides.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/convolutionalvsfullyconnected.png}
    \caption{Convolutional Layer}
    \label{fig:Convolutional Layer vs Fully Connected Layer}
\end{figure}
\subsection{Receptive Field}
Again considering images as signals, we can think of the Receptive Field as the part of the image that the Kernel is applied to. The Receptive Field is the area of the input image that has an effect on the output of the layer. This makes sense since the pixels in this case close to each other holds for other examples as well though are usually correlated 
and thus it makes sense to consider the once close to each other. This can also be seen in this figure from the slides.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/receptive_field.png}
    \caption{Receptive Field}
    \label{fig:Receptive Field}
\end{figure}
Here g3 is effected by h2, h3, h4, which are in turn also always influenced by the previous layer. Which would be x1, x2, x3, x4, x5. This means the receptive field grows large in Multilayer Networks, even if the Kernel is small. 

\subsection{Dilation}
Another way to make the Receptive Field rather large is Dilation. Again in the context of pixels and images(imho the most intuitiv). In a traditional convolution, a filter slides over the input image, and at each position, it computes the dot product between the filter weights and the image pixels. The dilation introduces gaps between the filter weights. This means that the filter will skip over certain pixels in the input, effectively increasing the distance between the pixels it considers. This means that we can maybe get a broader context. 

\subsection{Padding}
Padding also makes sense lets again as always consider a Grid of Pixels, if we apply a Kernel of say size 3. If we apply the Kernel to the first Pixel of the image on a side we would loose that pixel, this would also happen if our Kernel reaches the last pixel of the image. This means that we would loose Every Pixel in the left pixel row, as well as the right, making our output smaller or less Dimensional. 
But this is not what we want we want to keep the Dimensionality of the Image, this is where Padding comes into play. Padding is just adding zeros around the image, so that the Kernel can be applied to every Pixel as we would basically just loose the zeros, instead of ours actual Pixels. This would then be called 0-Padding. There are also other types of Padding, such as Mirroring the Signal at the Edges or Peridodic Padding.(Not explained further on the slides, so I wont do it either.)

\subsection{Stride}
We can reduce the Dimensionality of the Output by using a Stride. The Stride is the amount by which the Kernel is moved over the Image. If we have a Stride of 2, the Kernel is moved by 2 Pixels. This also means that the Receptive Field Grows as our "context" grows larger. If we reduce Dimensionality it is also called Downsampling. 
\subsection{Pooling} 
In Pooling we can look at neighbouring Pixels and take an average or the maximum value. This also lets us assess context better, after we have done that we can for example do a stride operation again.
\newpage
\subsection{Example Max Pooling}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/maxpooling.png}
    \caption{Max Pooling}
    \label{fig:Max Pooling}
\end{figure}
This is an example of Max Pooling, here we just divide our image into 2x2 squares and take the maximum value of the 4 Pixels, since we use stride 2 there are no overlaps. This reduces the Dimensionality as can be seen in the output. 

\subsection{Convolutional Layers with Several Channels}
Ok this is where the Mind Fuck starts a bit. Lets talk about images. Our image has its set of pixels now and we always naively assumed the Pixels to have a certain value. But usually they might have multiple Values, for example RGB etc. This poses a Problem though, how we do we compute those with our Kernel Matrix. The answer is we dont we need a bit of a more sophisticated approach. 
So lets introduce 4 Dimensional Tensors. 

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{images/4D_tensor.png}
    \caption{4D Tensor}
    \label{fig:4D Tensor}
\end{figure}
This figure depicts a 4D Tensor, lets break down what we need the Dimensions for, so first of all like before we need height and width of our Matrix. So we are in 2D Now, now all of those need to be applied to red green and blue, so now we are in 3D. Now we want to apply m filters for example to detect things such as  features like edges, textures, or colors, so for all of those we need different blocks of the 3D filters I just described. 
This is why we need the 4th Dimension. 

\subsection{Transpoed Convolution Layer}
This is the same process we have been discussing, for Convolution Layers, but in reverse. All the Concepts we learned can be applied to those as well in reverse. This is usefull for Decoders and Genrative Models.

\section{Batch Normalization}
We often have the issue that the input to the Activation Function is not centered around 0, this can lead to exploding or vanishing gradients. Now we learn about Batch Normalization in order to prevent that Problem. Batch Normalization can be applied to the Input but also the hidden layers. Ok if we take a look at the equation for Batch Normalization we see that we subtract the mean and divide by the standard deviation.
from a Matrix H which contains the activations for a Mini Batch. So $H' = \frac{H - \mu}{\sigma}$ So what does that do, well substracting the mean we center the activations around 0, and dividing by the standard deviation we scale the activations to have a standard deviation of 1. Forcing the activations to have a mean of 0 and a standard deviation of 1. It should be noted that the standard deviation also adds a really small number to prevent divison by 0. 

\subsection{Why is this usefull?}
If we consider activation Functions, such as the Sigmoid Function, if we input a very large number for exampe we get to a point where the function is almost flat, this means that the gradient is almost 0. This of course is problematic, thus normalizing makes perfect sense. In the Context of Inference, we need to keep track of a running average if the mean and standard deviation, 
so we can apply some kind of normalization to the test data.

\subsection{How we do it in Practice}
To my understanding this means that the Data might be more complex for us to adjust leave it normalized around zero, so we need to make it learn where it wants to be. This is done by introducing two new parameters, $\gamma$ and $\beta$. $\gamma$ is used to scale the normalized value and $\beta$ is used to shift the normalized value. Both of those are learned during training, we can always leave out the Bias, since $
\beta$ is pretty much the same but a bit more flexible. So now we calculate $\gamma \cdot H' + \beta$

\section{Convolutional Neural Networks Architectures}
TODO if someones to list them all here, too lazy for that, would be a bit of a prank if this comes in the exam.

\section{CNN visualization}
Probably makes more sense to read the slides, a lot of images. Would be weird to quizz on this, as it was never remotely part of the tasks or the practicals, as far as i can remember. 

\section{Transfer Learning}
We can do transfer learning if we for example have a classifier for cats, and now want to classify dogs. We cann use features of the cat network, to classify dogs. We could for example initialize the weights of the dog network with the weights of the cat network, this would probably give us a good starting point for gradient descent. 

\section{Autoencoders}
An autoencoder is an architecture for neural networks designed to efficiently compress input data, reducing it to its essential features, and then reconstruct the original input from this compressed representation. Compression happens as the encoder takes the input data and transforms it into a usually lower-dimensional representation. The reconstruction occurs in the decoder, which attempts to restore the original input data.

The name was chosen because the model learns to automatically and independently encode and then decode the data.
As this is just another Feed-Forward-Network, we can use all the techniques we have learned so far. This means we can train it using SGD and Backpropagation. Since we want the output to be close to the input we can minimize that giving us \[L(x, g(f(x)))\] where x is the input and $g(f(x)) $ is the output. The error function can for example use the L1 or L2 loss function. $g(f(x))$ will be referred as the rekonstruction 
$r$ moving forward.
\subsection{Goal and Usecases}
When we train an autoencoder, often the output itself is not what we are interested in, but rather the hidden representations, which are often the compressed lower dimensional version of the input data. This can be used for example for dimensionality reduction, or for example for denoising since it can learn to focus on the underlying 
structure of the data. It is also usefull for Self Supervised Learning ad Generative Models.(Not explained further on the slides, so I wont do it either.)

\subsection{Undercomplete, Complete and Overcomplete Autoencoders}
An undercomplete autoencoder is one where the hidden layer has fewer neurons than the input layer. This forces the autoencoder to learn a compressed representation of the data. A complete autoencoder
is one where the hidden layer has the same number of neurons as the input layer. This means that the autoencoder can learn to store the input data in the hidden layer. An overcomplete autoencoder is one where the hidden layer has more neurons than the input layer. This allows the autoencoder to learn multiple ways to represent the input data.(To not get a perfect 
reconstruction, we would need to introduce some kind of regularization, so no trivial solution is found.)

\section{Information Retrieval}
\subsection{Autoencoder vs PCA}
PCA is Principal Component Analysis, and what is basically does is it checks for correlated features and then tries to find a new set of uncorrelated features, thus reducing Dimensionality. We do this by finding the Principal Components which are the directions in which the Data varies the most. The biggest difference is that Autoencoders can learn none linear representations
, while PCA can only learn linear ones. We have to be carefull not giving the Autoencoder too much capacity, since it could just learn the Identity Function, effectivly copying the input to the output. Due to the universal approximation theorem, the Autoencoder can also learn noise and irrelevant features, which would overfit the data. If it doesnt have enough Capacity
it wont have enough flexibility to learn the underlying structure of the data.
\subsection{Information Retrieval}
Information Retrieval has the goal of finding entries in a Database, which are relevant to a query. This can be implemented efficiently in lower dimensional space priovided by the Autoencoder. We can force our hidden Code to be binary code, which would make it easier to compare the entries in the Database. We could save it in a Hash-Table where the Code-Vectors 
are the Keys and the Entries are the Values. We could then Flip a Bit in the Query and then compare the Code-Vectors to find the most similar entries. This is called Semantic Hashing.
\section{Regularization in Autoencoders}
Ideally we want our Autoencoder to choose a Dimensionality for the Data, which represents the complexity of the Data. So for Autoencoders we can also introduce Regularization. Besides not wanting to copy the Data, we also want to force Sparsity of the Representation and make the Model robust to Noise. A well regularized Autoencoder
should have bigger capacity, while also being able to learn the underlying structure of the Data.

\subsection{Sparse Autoencoders}
A sparse Autoencoder adds a Regularization term to the loss function, \[ L(x, r) + \Omega(h) \]. This is commonly used to learn features for other tasks
such as Classification. It has to be noted that this is not equivalent to the MAP Estimation. It can be seen as a preference for certain Functions. If we take a look we see some similarities to a generative Model with latent variables. Ok lets go through why that is. 
Given a model with visible variables \( x \) and latent variables \( h \), the joint probability distribution is:
\[
p_{\text{model}}(x, h) = p_{\text{model}}(x|h) \cdot p_{\text{model}}(h)
\]

% Log-Likelihood via Marginalization
The log-likelihood of the observed data \( x \) can be found by marginalizing over the latent variables \( h \):
\[
\log p_{\text{model}}(x) = \log \sum_h p_{\text{model}}(x, h)
\]

% Autoencoder Approximation
The autoencoder can be interpreted as an approximation by considering only the most probable value of \( h \) that maximizes:
\[
\log p_{\text{model}}(x, h) = \underbrace{\log p_{\text{model}}(x|h)}_{-L(x,r)} + \underbrace{\log p_{\text{model}}(h)}_{-\Omega(h)}
\]
The term $\log p_{\text{model}}(h)$ can for example be defined as the Laplace Prior. 

\subsection{Depth}
As we just have a Feed-Forward-Network, we can just add more layers making the network more powerfull. 

\subsection{Denoising Autoencoders}
We can build a Denoising Autoencoder by comparing the Loss with r. But this might lead to issues, since the 
Autoencoder (with enough capacity) will find the Identity Function. So we can add Noise to the Input, and then compare the Output to the original Input. This is called Denoising Autoencoder. So the loss $ L(x, r)$ is now defined with $r = g(f(\hat{x}))$ with $\hat{x}$ being the Noisy Input.

\subsection{Network Initialization with stacked denoising Autoencoders}
We can use denoising Autoencoders to pre train deep networks. The deep network training can be subdivided into Multiple steps. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/stacked_autoencoders.png}
    \caption{Stacked Autoencoder}
    \label{fig:Stacked Autoencoder}
\end{figure}

As visible in the image we keep on Stacking the Autoencoders. And after each step calculate the loss, and then Stack again. 
At the last Autoencoder we train the layer(which is a decoder) supervised and then we can fine tune the whole network by training the whole network supervised. (NOTE: I am really unsure here)
\section{Variational Autoencoders and Stochastic Autoencoders}
\subsection{Discrimative vs Generative Models}
\begin{itemize}
    \item Discriminative Models: Learn the conditional probability distribution \(Y^{*} = \arg \max_{Y} p(y|x) \) directly.
    \item Generative Models: A generative method models the conditional probabilities for the classes \( P(X|Y) \) and uses Bayes' theorem together with the prior \( P(Y) \) to find the posterior \( P(Y|X) \).

    The term generative refers to the fact that the prior \( P(Y) \) and the likelihood \( P(X|Y) \) can be used to generate new data by sampling from \( P(X) = \sum_Y P(X|Y) \cdot P(Y) \).
    
    The class labels are determined by:
    \[
    Y^* = \arg\max_Y P(Y|X) = \arg\max_Y \frac{P(X|Y) \cdot P(Y)}{P(X)}
    \]
    Since \( P(X) \) is constant for all classes, this simplifies to:
    \[
    Y^* = \arg\max_Y P(X|Y) \cdot P(Y)
    \]
\end{itemize}

\section{Stochastic Autoencoders}
Thus far we have only considered deterministic Autoencoders, but we can also introduce probabilistic Functions. We could say that we now have $p_{encoder}(h|x)$  and $p_{decoder}(x|h)$. 

\subsection{Stochastic Encoder and Decoders}    
Since we are using Neural Nets as before our strategy doesnt really change, the only difference in Terms of the Loss is that, $x$ is not just our input, but also the target of the output. This is why we get the Decoder which tries to reconstruct the input given the hidden code and thus we need to minimize negative log likelihood of this reconstruction $
- \log p_{decoder}(x|h)$. Regarding the output, if x follows a normal distribution, we can minimize the mean squared error, because the NN needs to predict the mean of the distribution. If x follows a Bernoulli distribution, which has 2 outcomes, Sigmoid makes sense, as you might remember it is just a value between 0 and 1. This is equivalent to minimizing the binary cross-entropy loss, which measures the difference between 2 
probability distributions. And then we also have the Case of Multiple Class Labels, in which case we use Softmax, because the sum of all probabilities is 1. We again can minimize the cross entropy loss.  As a short reminder because I forgot as well the formulas written out to show that the cross entropy is indeed the same as the negative log likelihood.

The likelihood \( L \) of the observed data given the predicted probabilities is:
\[
L = \prod_{i=1}^{N} p_i^{y_i} (1 - p_i)^{1 - y_i}
\]

The log-likelihood \( \log L \) is:
\[
\log L = \sum_{i=1}^{N} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\]

The negative log-likelihood is then:
\[
-\log L = -\sum_{i=1}^{N} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\]


The cross-entropy \( H(p, q) \) for binary classification is given by:
\[
H(p, q) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\]

When comparing this to the negative log-likelihood, we see that they differ only by the constant factor \( \frac{1}{N} \), which represents the average over all instances. Since this constant factor does not affect the minimization process, minimizing the cross-entropy is equivalent to minimizing the negative log-likelihood.


\section{Variational Autoencoders}
Instead of directly producing a laten representation the encoder outputs parameters to a probability distribution, from which we then sample. However this poses a problem since we cannot backpropagate through the sampling process, because sampling is not differentiable. 
\subsection{Reparametrization Trick}
The reparametrization trick is a way to make the sampling process differentiable. Instead of sampling from the distribution directly, we sample from a standard normal distribution and then transform the samples to the desired distribution, so our latent variable is now $z = \mu + \sigma \cdot \epsilon$ where $\epsilon$ is sampled from a standard normal distribution. This way we can backpropagate through the sampling process.
\subsection{Variational Autoencoder Loss}
The loss function of a variational autoencoder consists of two parts: the reconstruction loss and the KL divergence. The KL Divergence which was introduced in chapter 2.15 is a measure of how much one probability distribution differs from a second one.  The loss function is then given by: \[
L = \lambda L(x, r) + D_{KL}(q(h|x) || p(h)) 
\]
Ok lets break this down. Since $q(h|x)$ is a normal distrubution, we can calculate the KL Divergence analytically, this was I think a super long proof on one theoretical task, how ever considering the length I am going to doubt that this is relevant to the exam and skip it. Ok first we have a look at the decoder. This Decoder is a probability function 
given by $p(x|h) $ Which means that given a hidden representation, we return a probability distribution over the Data $x$. This conditional probability distribution is given by a Normal Distribution, with mean $\mu$ and standard deviation $\sigma$. \[
p(x|h) = \mathcal{N}(x|\mu_{dec}(h), \sigma^2) \] h follows a normal distribution as well, so the h is drawn from a distrubution with mean 0 and standard deviation 1. Then $p(x)$ is also fairly simple to calculate, all we have to do is consider all values of x given h and the respective probabilities of h, therefore with Marginalization
we get \[
p(x) = \int p(x|h) p(h) dh \] 

\textbf{Problem:} This expression is difficult to compute because the integral \( p(x) = \int p(x|h)p(h) \, dh \) is hard to calculate (if we try to estimate the integral by sampling from \( p(h) \), \( p(x|h) \) is zero for most values).

 Can we sample \( h \) in such a way that \( p(x|h) \) does not frequently yield small values? (i.e., instead of sampling from \( p(h) \), we prefer to sample from \( p(h|x) \)). With some tricks, this is possible...

Instead of maximizing \( p(X) \), we use the log-trick and maximize \( \log(p(X)) \):
\[
\log(p(X)) = \log \left( \prod_{x \in X} p(x) \right) = \sum_{x \in X} \log(p(x))
\]

This is feasible because the logarithm is monotonically increasing. Instead of setting \( p(x) = \int p(x|h)p(h) \, dh \), we add another term and maximize:
\[
\sum_{x \in X} \log(p(x)) - D_{KL} (q(h|x) \| p(h|x))
\]
where \( D_{KL} \) denotes the Kullback-Leibler divergence.

Since the Kullback-Leibler divergence \( D_{KL} \) is always \( \geq 0 \), and is \( 0 \) if \( q(h|x) = p(h|x) \), this provides a lower bound for the log-evidence. By maximizing this expression and having \( q(h|x) \) approach \( p(h|x) \), the \( D_{KL} \)-term approaches 0, and we maximize the original \( \log(p(x)) \) term.

We can simplify the ELBO as follows:
\[
\text{ELBO} = \log(p(x)) - D_{KL}(q(h|x) \| p(h|x))
\]
\[
= \log(p(x)) - \int q(h|x) \left( \log(q(h|x)) - \log(p(h|x)) \right) \, dh
\]

Using Bayes' theorem:
\[
= \log(p(x)) - \int q(h|x) \left( \log(q(h|x)) - \log(p(x|h)) - \log(p(h)) + \log(p(x)) \right) \, dh
\]

\[
= \int q(h|x) \log(p(x|h)) \, dh - \int q(h|x) \left( \log(q(h|x)) - \log(p(h)) \right) \, dh
\]
\[
= \int q(h|x) \log(p(x|h)) \, dh - D_{KL}(q(h|x) \| p(h))
\]

Instead of maximizing the ELBO, we usually minimize the negative ELBO, leading us back to the original loss function:
\[
L = - \int q(h|x) \log(p(x|h)) \, dh + D_{KL}(q(h|x) \| p(h))
\]

Compared to the original integral \( p(x) = \int p(x|h)p(h) \, dh \), \( \int q(h|x) \log(p(x|h)) \, dh \) can be efficiently sampled because \( q(h|x) \) approximates \( p(h|x) \). For a given input image \( x \), the encoder network first calculates \( \mu_i \) and \( \sigma_i \) for \( q(h|x) = N(h|\mu_i, \sigma_i) \). Differentiable values for \( h \) can then be sampled from this distribution using the reparameterization trick. These values are used in the decoder to compute \( p(x|h) \). If we use a Normal distribution for \( p(x|h) \), we end up with an L2 loss for \( L(x, r) \) with \( r = \mu_{\text{dec}}(h) \).
 This was quite a long and detailed explanation, I am not sure if I get it myself, and I really doubt this is relevant for the exam, but I am going to leave it in here, and the core concepts hopefully came across.

 \subsection{Pros and Cons of Variational Autoencoders}
 \begin{itemize}
    \item \textbf{Pros:}
    \begin{enumerate}
        \item In VAEs close Proximity in the latent space corresponds to close proximity in the output space. This is particularly usefull, because it makes it easier to interpolate between different data points. 
        \item VAEs can be used to generate new data points, by sampling from the latent space, this makes VAEs usefull for generative tasks.    
    \end{enumerate}
    \item \textbf{Cons:}
    \begin{enumerate}
        \item VAEs outputs are more blurry compared to conventional Autoencoders, because the laten space tries to follow a distribution. 
        \end{enumerate}
 \end{itemize}
 \newpage
 \section{U-Net}
 For the funsies here is an image of a so called U-Net, apparently it is used for Image Segmentation Tasks. 
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.9\textwidth]{images/unet.png}
        \caption{U-Net}
        \label{fig:U-Net}
    \end{figure}
\section{Autoregressive Models} 
Lets start by having a grid, of Pixels with size nxn, and look at this example in the Figure below.  
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{images/autoregressive.png}
    \caption{Autoregressive Model}
    \label{fig:Autoregressive Model}
\end{figure}
In Autoregressive models, we look at this grid and look at the pixels, as one long array, and in this array the pixels only depend on the pixels before them. So if we look take a look at the figure we see the pixel $x_{t}$ only depends on pixels before that, so $x_{t-1}$ and $x_{t-2}$ and so on, until $x_{0}$. If we consider this p(x) 
is given by the product of the conditional probabilities of the pixels given the pixels before them. So $$p(x) = \prod_{i=1}^{n^2} p(x_{t}|x_{1}, x_{2}, ..., x_{t-1})$$ Here we can again Train the Model using negative log likelihood, we then minimize $\theta$ so that $$
\theta = \arg \min_{\theta} - \sum_{t = 1}^{n^2} \log p(x_{t}|x_{1}, x_{2}, ..., x_{t-1})$$
\subsection{PixelCNN}
A NN with the properties discussed before this, can be implemented by using Masks to make sure that the pixel only depends on the pixels before it. The architeturce is flawed though, because using masks can lead to blindspots, this is why we devide the receptive field into a horizontal and vertical stack. (TODO Explain this better! 
I dont really get the images on the slides)
\subsection{Inference Autoregressive Models}
\begin{algorithm}
    \caption{Generating Images with an Autoregressive Model}
    \begin{algorithmic}[1]
    \State Initialize the image $x$ with 0.
    \For{$i \in \{1, \ldots, n^2\}$}
        \State Compute the probabilities $p(\cdot \mid x_{t-1}, \ldots, x_1)$ with PixelCNN.
        \State Sample the pixel value $x_t \sim p(\cdot \mid x_{t-1}, \ldots, x_1)$.
    \EndFor
    \end{algorithmic}
    \end{algorithm}
    \subsection{Pros and Cons of Autoregressive Models}
    \begin{itemize}
        \item \textbf{Pros:}
        \begin{enumerate}
            \item Can be used with Discrete and Continuous Data.
            \item Allows to calculate the probability if an image belongs to a Dataset directly. 
            \item Training is stabil
        \end{enumerate}
        \item \textbf{Cons:}
         \begin{enumerate}
            \item Slow to generate Samples, since we have to wait for the previous pixel to be generated. 
            \item The architecture needs to maintain autoregressive properties during trainin. 
            \end{enumerate}
    \end{itemize}
\section{Generative Adversarial Networks (GAN)}
Ok, now we want a real battle of CNNs and introduce GAN. We have two Networks, one generates random samples, and the other one tries to distinguish between real and fake samples.  So we have a so called Generator Network, and discriminator Network. 
\subsection{Training a GAN}


The objective function of a GAN is the binary cross-entropy:
\[
J_{\text{GAN}} = \underbrace{\log D_{\theta_d}(x)}_{\text{Probability that } x \text{ is real}} + \underbrace{\log(1 - D_{\theta_d}(G_{\theta_g}(z)))}_{\text{Probability that } G_{\theta_g}(z) \text{ is fake}}
\]

During training, we alternate between optimizing the discriminator and the generator:

\[
\theta_d^* = \arg \max_{\theta_d} J_{\text{GAN}}
\]
\[
\theta_g^* = \arg \min_{\theta_g} J_{\text{GAN}}
\]
The networks is done Training when the Diskriminator can no longer distinguish between real and fake samples, thus outputs 0.5 for all samples.

The algorithm does basically that, but I am too lazy to tex, TODO tex the algorithm. 

\subsection{Pros and Cons of GANs}
\begin{itemize}
    \item \textbf{Pros:}
    \begin{enumerate}
        \item Allows for generation of new images
        \item Highly realistic images can be generated
    \end{enumerate}
    \item \textbf{Cons:}
    \begin{enumerate}
        \item We are training 2 CNN at the same time, which can take a lot of time.
        \item The Training is unstable due to the minmax game between the Generator and the Discriminator.
        \item Mode Collaps, the Generator might find specific features that fool the Discriminator, but dont represent the whole data, eg only generating Seals, even thought the Dataset has more animals. 
        \end{enumerate}
\end{itemize}
\section{Flow Based Generative Model}
The following text was rewritten by chatgpt(I cant write this well lol)
Suppose we have a random variable \( z \) with a simple distribution, such as a standard normal distribution. We now want to transform this random variable into a more complex distribution using a function \( x = f_{\theta}(z) = f_k \circ \ldots \circ f_2 \circ f_1(z) \), which is a composition of invertible functions. This gives \( f \) the nice property that it is invertible due to it being bijective. For any \( x \), we have exactly one \( z \) that could be sampled to produce it.

To start, we need the change of variables formula. With \( f : Z \rightarrow X \) being a bijective function, and \( p_{\theta}(z) \) defined over \( z \in Z \), we get the formula:
\[
p_{\theta}(x) = p_{\theta}(f^{-1}(x)) \left| \det \left( \frac{\partial f^{-1}(x)}{\partial x} \right) \right|
\]

Since we have a specific \( z \) associated with each \( x \):
\[
p_{\theta}(x) = p_{\theta}(z) \left| \det \left( \frac{\partial z}{\partial x} \right) \right|
\]

Let's specify the prior distribution \( p_{\theta}(z) \) as a normal distribution:
\[ 
p_{\theta}(z) = N(z | 0, I) 
\]

Thus, for the distribution over \( x \), we have:
\[
p_{\theta}(x) = p_{\theta}(z = f_{\theta}^{-1}(x)) \left| \det \left( \frac{\partial f_{\theta}^{-1}(x)}{\partial x} \right) \right|
\]

In higher dimensions, this involves the determinant of the Jacobian of \( f_{\theta} \):
\[
p_{\theta}(x) = p_{\theta}(z = f_{\theta}(x)) \cdot \left| \det(J_{f_{\theta}}(x)) \right|
\]

Our goal is to maximize the likelihood of the data \( x_i \):
\[
\theta^* = \arg \max_\theta \prod_i p_\theta(x_i)
\]

Taking the log, this becomes:
\[
\theta^* = \arg \max_\theta \sum_i \log(p_\theta(x_i))
\]

Substituting the change of variables formula, we get:
\[
\theta^* = \arg \max_\theta \sum_i \left[ \log(p_\theta(z = f_\theta(x_i))) + \log \left| \det(J_{f_\theta}(x_i)) \right| \right]
\]

To achieve this, we need to design an invertible neural network that can compute both \( g_\theta \) and \( f_\theta \) efficiently. Additionally, we need a way to compute the Jacobian determinant \( \det(J_{f_\theta}(x)) \) efficiently.

\subsection{Affine Coupling Layer}
To make everything easier for us lets introduce affine coupling layers. \begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{images/affine_coupling_layer.png}
    \caption{Affine Coupling Layer}
    \label{fig:Affine Coupling Layer}
\end{figure}
This affine coupling layer example is a simple example yet shows why it is so effective for us, lets break it down. First we split the input into two parts, $x_1$ and $x_2$. We then already see that $y_1 = x_1 $ which I will get to later lets first focus on $y_2$. $y_2$ is computed by $x_2 \cdot \exp(s(x_1)) + t(x_1)$. This partition computes
$y_{2}$ only considering x2 and s, t which only depend on x1. Since $x_1$ is the identical to $y_1$ we can just use $y_1$ as $x_1$, therefore creating $$y_2 =  x_{2} \cdot \exp(s(y_1)) + t(y_1)$$ This means the inverse of f is given by $$x_1 = y_1$$ and $$x_2 = (y_2 - t(y_1)) \cdot \exp(-s(y_1))$$. Lets break down the Inverse function of $y_2$
First we substract the shift we applied to $y_2$ and then we divide by the scaling factor(here -s to make it easier to read). 

\subsection{Affine Coupling Layer Determinante of the Jacobian}
The Jacobian matrix \(J_f\) of the transformation can be written as a block matrix:
\[
J_f = 
\begin{pmatrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} \\
\frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2}
\end{pmatrix}
=
\begin{pmatrix}
I & 0 \\
\frac{\partial y_2}{\partial x_1} & \text{diag}(\exp(s(x_1)))
\end{pmatrix}
\]
Lets examine why this makes sense closer, since $y_1 = x_1$ the first entry of the Jacobian is the Identity Matrix, due to $\frac{\partial y_1}{\partial x_1}$ being the first entry. Since $y_1$ does not depend on $x_2$ it is zero. The third entry is annoying but fck that we will leave it for now which makes sense as we will see in a bit. The last also makes sense if we consider. 
The lower right block \(\frac{\partial y_2}{\partial x_2}\) of the Jacobian matrix is a diagonal matrix because the transformation for \(x_2\) is:
\[
y_2 = x_2 \odot \exp(s(x_1)) + t(x_1)
\]
where \(\odot\) denotes element-wise multiplication, and \(s(x_1)\) and \(t(x_1)\) are functions derived from \(x_1\).

To compute the partial derivatives:
- For \(i = j\):
  \[
  \frac{\partial y_{2,i}}{\partial x_{2,i}} = \exp(s_i(x_1))
  \]
- For \(i \neq j\):
  \[
  \frac{\partial y_{2,i}}{\partial x_{2,j}} = 0
  \]

Since each element \(y_{2,i}\) only depends on the corresponding element \(x_{2,i}\) and not on any other elements of \(x_2\), the Jacobian \(\frac{\partial y_2}{\partial x_2}\) is a diagonal matrix:
\[
\frac{\partial y_2}{\partial x_2} = \text{diag}(\exp(s(x_1)))
\]
Now that every element makes sense lets continue with some very usefull tricks.
For a \(2 \times 2\) block matrix 
\[
\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}
\]
where either \( B = 0 \) or \( C = 0 \), it follows from the generalized rule for the determinant:
\[
\text{det} \begin{pmatrix}
A & 0 \\
C & D
\end{pmatrix}
= \text{det} \begin{pmatrix}
A & B \\
0 & D
\end{pmatrix}
= \text{det}(A) \cdot \text{det}(D)
\]

The determinant of the identity matrix \( A \) is 1 and the determinant of a diagonal matrix \( D \) is the product of its diagonal entries. Therefore, the determinant of the Jacobian of the affine coupling layer is:
\[
\text{det}(J_f) = 1 \cdot \exp \left( \sum_j s(x_1)_j \right).
\]
This rather through dive into Affine Coupling Layers was usefull, as we now compute the determinant of the Jacobian very easily making this model feasable. 

\subsection{Coupling Layers}
We now face one more problem if we just align coupling layers we will run into the problem, that the Normalizing Flow will not learn anything as it is an Identity Function to the first partition $x_1$. To solve this  we can permute partitions in between layers, this can either be done by like shuffeling the inputs or sth like that, which would be the naive approach. We can also use 1x1 convolutions which learn which permutation maximizes performance.

\subsection{Pros and Cons of Flow Based Generative Models}
\begin{itemize}
    \item \textbf{Pros:}
    \begin{enumerate}
        \item Allows for efficient Sampling of new Data 
        \item We can reflect the variety in the training data and thus produce more variety in the generated data.
        \item Allows for calculation of latent representation of the data.
        \item Integrate into other Models to ensure a good prior, for example in VAE we could use a Flow Based Generative Model to ensure a distribution that makes more sense then a normal distribution.
    \end{enumerate}
    \item \textbf{Cons:}
     \begin{enumerate}
        \item Network architecture needs to be invertible
        \item The image quality is lower than in GANs or Diffusion Models
        \item Idk but the Slides say GANs are the way to go, so I guess they are not as good as GANs.
        \end{enumerate}
        
\end{itemize}
\newpage
\section{Diffusion Models}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/diffusion.png}
    \caption{Diffusion Model}
    \label{fig:Diffusion Model}
\end{figure}
Another way to create images is using a diffusion Model. We will slowly deconstruct the image as seen in the Figure, at the same time a NN is trained to do the exact opposite of that. In the figure we can see the NN $p_{\theta}(x_{t-1}|x_t)$ fighting the diffusion process given by $p_{\theta}(x_t|x_{t-1})$. 
\subsection{Forward Diffusion Process}

Starting from an initial observation \( x_0 \sim p(x) \), the forward diffusion process generates a sequence of increasingly noisy samples \( x_1, x_2, \ldots, x_T \). Lets dicuss this in more detail: 

\begin{itemize}
    \item \textbf{Adding Noise Over Time:}
    \begin{itemize}
        \item At each step \( t \) in the sequence, we add a small amount of Gaussian noise to the sample from the previous step.
        \item The mean of the sample is slightly reduced, and the noise level is controlled by a parameter \( \beta_t \) that increases over time.
    \end{itemize}

    \item \textbf{Defining the Noise Schedule:}
    \begin{itemize}
        \item The noise level at each step is determined by a "variance schedule" \( \beta_t \), where \( \beta_t \) increases as \( t \) gets larger.
        \item This means that the samples get progressively noisier as we move through the steps.
    \end{itemize}

    \item \textbf{Mathematical Formulation:}
    \begin{itemize}
        \item The forward process can be described by:
        \[
        q(x_{1:T} | x_0) = \prod_{t=1}^{T} q(x_t | x_{t-1}), \quad q(x_t | x_{t-1}) = \mathcal{N}(x_t | \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
        \]
        \item Here, each \( x_t \) is drawn from a normal distribution with a mean of \( \sqrt{1 - \beta_t} x_{t-1} \) and variance \( \beta_t \).
    \end{itemize}

    \item \textbf{Combining Multiple Steps:}
    \begin{itemize}
        \item We can combine multiple diffusion steps into one using a compound factor.
        \item Let \( \alpha_t = 1 - \beta_t \) and 
        \[
        \bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s,
        \]
        then:
        \[
        q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t)I)
        \]
    \end{itemize}

    \item \textbf{Effect Over Time:}
    \begin{itemize}
        \item As the process continues, the original sample \( x_0 \) gradually loses its distinct features and becomes increasingly noisy.
        \item IThis will of course be a standard normal distribution at the end.
    \end{itemize}
\end{itemize}

\subsection{Reversing the Diffusion Process}

To generate new samples \( x_0 \sim p(x) \) from a latent variable \( x_T \sim \mathcal{N}(0, I) \), we need to reverse the diffusion process and sample from \( q(x_{t-1} | x_t) \). If \( \beta_t \) is small enough, \( q(x_{t-1} | x_t) \) will also be normally distributed. However, directly estimating \( q(x_{t-1} | x_t) \) is impractical because it would require the entire dataset. Instead, we use a neural network \( p_\theta(x_{t-1} | x_t) \) to approximate these conditional probabilities in a diffusion model. The reverse diffusion process can then be conducted as follows:

\begin{itemize}
    \item \textbf{Training the Neural Network:}
    \begin{itemize}
        \item A neural network \( p_\theta(x_{t-1} | x_t) \) is trained to approximate the conditional probabilities \( q(x_{t-1} | x_t) \).
        \item The model learns the mean \( \mu_\theta(x_t, t) \) for the distribution.
        \item The variance \( \sigma_t^2 \) is set to a predefined value (e.g., \( \sigma_t^2 = \beta_t \)) and is not learned.
    \end{itemize}

    \item \textbf{Reverse Diffusion Process:}
    \begin{itemize}
        \item The reverse process is defined as:
        \[
        p_\theta(x_{0:T}) = p_\theta(x_T) \prod_{t=1}^{T} p_\theta(x_{t-1} | x_t)
        \]
        \item Where:
        \[
        p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1} | \mu_\theta(x_t, t), \sigma_t^2 I)
        \]
    \end{itemize}
    
    \item \textbf{Generating New Samples:}
    \begin{itemize}
        \item Start with a sample \( x_T \) from \( \mathcal{N}(0, I) \).
        \item Iteratively sample \( x_{t-1} \) from \( p_\theta(x_{t-1} | x_t) \) until \( t = 1 \).
        \item The final sample \( x_0 \) will be drawn from \( p(x) \).
    \end{itemize}
\end{itemize}
\subsection{Objective Funtion}
Ok i have no clue what happened on those slides this is the result of some paper from 2020. TODO wtf is happening here.
\[
B_{\text{simple}} := \sum_{t=2}^{T} \mathbb{E}_{t, x_0, \epsilon} \left\| \epsilon - \epsilon_\theta \left( \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t \right) \right\|^2
\]
\subsection{Architecture}
We can use a U-Net to model the diffusion model. But this is kinda of what ever because we just need to map one image to another. 

\subsection{Pros and Cons of Diffusion Models}
\begin{itemize}
    \item \textbf{Pros:}
    \begin{enumerate}
        \item Used currently to generate the most realistic images
        \item We can reflect the variety in the training data and thus produce more variety in the generated data.
        \item We don't need a particular architecture. 
    \end{enumerate}
    \item \textbf{Cons:}
     \begin{enumerate}
        \item The Training is rather slow, due to the fact that we need to evaluate the Model in every iteration, this also makes creation of images slow. 
    \end{enumerate}
    \end{itemize}
\section{Transformer Networks}
    \subsection{Introduction to Natural Language Processing}
    We want to process natural language, spoken or written by humans now. A Transformer looks like this full image, however we will break it down into smaller parts. 
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.7\textwidth]{images/transformer.png}
        \caption{Transformer}
        \label{fig:Transformer}
    \end{figure}    
    \subsection{Transformers}
    The goal of a Transformer Network is to map input to output, we could take a translation from, English to German for example. Ok what is our first step now? First we need to encode the input, this is what we call the Embedding. 
    \subsection{Embedding}
    \newpage
    Human Language is often not fit as input, since using single letters would lead to a huge input space, this is why we use Embeddings. Elements of this embedded sequence are called Tokens. Regarding the image from the Slides, we have an example sentence, which we encode into a sequence of ints, which we then map to a vector with random initialized values. The sequence of vectors can be 
    seen as matrix, where each row is a vector corresponding to a token in the sentence for simplicity imagine the tokens as words. Those vectors are not static though. 
    \subsection{Gradient Descent on Embeddings}
    The Embeddings are now considered part of the models parameters. During training we can adjust the embeddings to minimize the loss function. Since we dont use all tokens at the look up table, only a subset of the vectors needs to be updated, since the other gradients are zero (i guess we would rather use $-\infty$).
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/input_embeddingg.png}
    \caption{Embedding}
    \label{fig:Embedding}
\end{figure}    
\subsection{Positional Encoding}
Next up in the figure of a Transformer Network is Positional Encoding, as you can see we add something onto our vectors, but what exactly? We add a sinusoidal function to the vectors, this vector looks as follows \[
\gamma(p) = \left[
\sin \left( \frac{p}{10000^{\frac{2i}{d_{model}}}} \right), 
\cos \left( \frac{p}{10000^{\frac{2i}{d_{model}}}} \right), 
\sin \left( \frac{p}{10000^{\frac{2(i+1)}{d_{model}}}} \right), 
\cos \left( \frac{p}{10000^{\frac{2(i+1)}{d_{model}}}} \right), 
\ldots
\right]
\]
This is like adding a frequency to the vectors, so the Neural Net can distinguish between the positions. One might think about why we dont just make the position a trainable parameter, that would work, but the authors of the paper preffered Sinuoid-Encoding allows for extrapolation for longer sequences this means we can expand on longer sequences(I think) thus making 
the model more general.

\subsection{Multi-Head Attention Mechanism}

Next, we'll dive into the Multi-Head Attention Mechanism, inspired by the excellent 3Blue1Brown video in his Deep Learning Series on Transformers. This section introduces some complex concepts, so let's break down the formula for attention as shown in the slides:

\[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \]

The first question is: What are \(Q\), \(K\), and \(V\), and why do we need them?

\begin{itemize}
    \item \textbf{Queries (Q)}: Think of queries as questions posed by tokens. For example, in the sequence \textit{I am a green happy tree}, the word \textit{tree} might query for adjectives describing it. We obtain the query vector by multiplying the embedding vector with a learned matrix. This matrix helps form questions related to the tokens.
    
    \item \textbf{Keys (K)}: Keys are used to match the questions (queries) with potential answers. By multiplying the embedding vector with another learned matrix, we get the key vector. If the key and query vectors are related, their dot product will be large. For instance, the dot product between the key and query for \textit{green} and \textit{tree} would be high.
    
    \item \textbf{Values (V)}: Values provide the actual content or answer. By multiplying the embedding vector with yet another learned matrix, we obtain the value vector. This vector is then used to refine our embeddings.
\end{itemize}

Heres how the attention formula works:

\begin{enumerate}
    \item \textbf{Compute Dot Product}: Calculate the dot product of queries and keys. This measures how well each key matches a query.
    \item \textbf{Scale by \( \sqrt{d_k} \)}: Divide by the square root of the key dimension \( d_k \) for numerical stability, preventing issues with large dot product values when applying softmax.
    \item \textbf{Apply Softmax}: Apply the softmax function to obtain attention weights. These weights determine how much focus each token should get.
    \item \textbf{Weighted Sum}: Multiply the attention weights by the value vectors and sum them up to get the final output.
\end{enumerate}

This process is parameterized by the learned Query, Key, and Value matrices.

Finally, to prevent the decoder from looking ahead and using future information (cheating), we mask the values of positions that have not been reached yet.

By understanding this process, the attention formula becomes clearer:

\[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \]


\subsection{Feed-Forward Neural Networks}
So what is still missing from the Transformer Network? One aspect is the Feed Forward Node on the Graph from the Start. This is a 2 Layer MLP(Multi Layer Perceptron). The first layer projects the input from a lower dimension to a higher dimension, the second layer then projects the input back to the original dimension. 
Between the two layers we use a ReLU activation function, to inroduce non linearity. This is done to capture more complex features of the data before passing it into the next Multi-Head Attention Block. 
\subsection{Residual Connections and Layer Normalization}
Residual connections in neural networks, introduced by ResNets, help mitigate the vanishing gradient problem by allowing the input to bypass certain layers and be added directly to the output. This creates a shortcut that facilitates better gradient flow and more stable training, especially in very deep networks. Instead of Batch Normalization, which normalizes each feature across the entire batch, Layer Normalization is used in Transformers, normalizing across the features for each individual sample, making it more suitable for varying batch sizes. 

\subsection{Transformers in Computer Grpahics and Vision}   
We dont really want text, Transformer are currently the best Models we have, so we also want to use them for Computer Vision and Graphics.

\section{Image-GPT}
The Imaage-GPT uses a Transformer to learn Representation of important Features of images. The Training consists of two steps, first we train a transformer to extract important featurs. Then we use pretrained Network for a Classification Task for the images. 
\subsection{Image Embedding}
The image needs to be transformed into a sequence of tokens. To begin we can create a vector of the image data with the size corresponding to the image size. For higher resolutions this is not practical, as the Attention Mechanism is in \(O(n^2 \cdot d)\), so with a resolution of 256x256 we would already have $256^4$ entries.  Scaling the image down is possible but not below 32x32 pixels, so humans can interpret it. Color depth can be 
reduced to 9 bits by using clustering algorithms. Here in this case we actually use positional encdoing vectors as parameters, compared to before. 

\subsection{Autoregressive Pre-Training}


The first pre-training strategy for Image-GPT is based on an autoregressive loss. The probability in an autoregressive model is given by:

\[
p_{\theta}(x) = \prod_{i=1}^{n} p_{\theta}(x_i | x_{i-1}, \ldots, x_1)
\]

During autoregressive training, the Transformer is trained to maximize the likelihood of the training sequences (or equivalently minimize the negative log-likelihood):

\[
L_{AR} = \mathbb{E}_{x \sim X} [-\log p_{\theta}(x)]
\]

To maintain the autoregressive property, as in the decoder of the Transformer architecture, a mask is applied to the attention scores.

\subsubsection{BERT-Based Pre-Training}

The second pre-training strategy is based on a BERT (Bidirectional Encoder Representations from Transformers) objective. Originally, BERT in the context of NLP is designed to learn how to fill in missing words in a sentence, where 15 \% of the words are missing. Here, the goal is to fill in missing pixels in an image.

A mask \( M \) is sampled, which includes a token \( i \) with a probability of 0.15. The model is then trained to maximize the likelihood of the masked tokens given the unmasked tokens:

\[
L_{BERT} = \mathbb{E}_{x \sim X} \mathbb{E}_{M} \left[ -\log p(x_i | x_{[1,n] \setminus M}) \right]
\]

During evaluation, a mask must also be used to avoid a shift in the distribution of input values. However, this mask can reduce the classifier's performance. To mitigate this, five independent masks are drawn, and the class with the highest average probability is predicted.

\subsection{Fine-Tuning}
TODO: Explain fine tuning
\subsection{Linear Probing}
TODO: Explain Linear Probing

\section{Vision Transformer}

Since the following are just facts for different architecutres I will just list them here and their proberties, but you can check out the slides for more or less the exact same Info in German, the following sections are brought to you by a transformer ;D for completness. 

Transform the input image \( x \in \mathbb{R}^{H \times W \times C} \) into 2D patches \( x_p \in \mathbb{R}^{N \times P^2 \times C} \), where:
\begin{itemize}
    \item \( H, W \) are the height and width of the image.
    \item \( C \) is the number of channels.
    \item \( N = \frac{HW}{P^2} \) is the number of patches (e.g., 16  16).
    \item \( P \) is the resolution of a single patch.
\end{itemize}
These patches are then projected linearly onto latent vectors ("embeddings") with dimensionality \( D \).

\subsection{Positional Embedding}

Learnable vectors, called positional embeddings, are added to the patch embeddings to encode positional information.

\subsection{Transformer Encoder}

The Transformer Encoder is a standard Transformer, as discussed previously, used to process the sequence of patch embeddings.

\subsection{Class Token}

A special input token, known as the class token, is introduced. The final state of this token is used as the input to the MLP for classification.

\subsection{MLP Head}

The Multi-Layer Perceptron (MLP) head is used for predicting class labels:
\begin{itemize}
    \item During pre-training on a large dataset, it includes a hidden layer.
    \item During fine-tuning on a smaller dataset, it consists of a single linear layer.
\end{itemize}
\subsection{TODO Swin}
\subsection{TODO CLIP}
\section{Neural Radiance Fields (NeRF)}
Now we want to synthesize novel views of a scene from a set of images. 
\subsection{Plenoptic Function}
The plenoptic function is a 7D function that describes everything visible in a scene. It is defined as $$P: (\theta, \phi, \lambda, t, V_x, V_y, V_z) \rightarrow \mathbb{R}$$
Ok here the slides are pretty good actually, I recommend checking them out, but I will try to explain it here as well. $\theta$ and $\phi$ are the angles of the camera, $\lambda$ is the wavelength of the light, $t$ is the time, $V_x, V_y, V_z$ are the coordinates of the point in the scene.
\subsection{3D Representation}
We have many ways of representing 3D scenes here are some examples. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/representation.png}
    \caption{3D Representation}
    \label{fig:3D Representation}
\end{figure}

\end{document}